{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "crawer",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "11h2gvU2w0w-cWs1O2ciOwdgK4Wi6qndj",
      "authorship_tag": "ABX9TyMA9XSEwRC2m2p3pQ2qvG2B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/reic/colab_python/blob/main/crawer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjvmI-ML-U4Y"
      },
      "source": [
        "# 網路爬蟲與多執行緒練習\n",
        "\n",
        "未穩定的版本"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcS6qBgpjmpr",
        "cellView": "form"
      },
      "source": [
        "#@title UU看書 專用(多執行緖)\n",
        "#@markdown 還在修正的程式，可以直接從這一個區塊執行\n",
        "\n",
        "\n",
        "import requests\n",
        "import os,re\n",
        "from bs4 import BeautifulSoup\n",
        "import concurrent.futures\n",
        "\n",
        "try:\n",
        "  os.mkdir(\"/content/tmp\")\n",
        "except:\n",
        "  print(\"目錄已存在\")\n",
        "os.chdir(\"/content/tmp\")\n",
        "os.system(\"rm -fr *\")\n",
        "\n",
        "\n",
        "def get_html(urls):\n",
        "  [title,art_url]=urls\n",
        "  art_id=art_url[art_url.rfind(\"/\")+1:-5]\n",
        "  \n",
        "  soup=BeautifulSoup(requests.get(art_url).text)\n",
        "  content=soup.find(name=\"div\",id='contentbox')\n",
        "  print(art_id)\n",
        "  text=f\"{title}\\n\\n\"\n",
        "  context=str(content).replace(\"\\n\",\"\").replace(\"\\r\",\"\")\n",
        "  context=context.replace(\"\\xa0\\xa0\\xa0\\xa0\",\"\")\n",
        "  context=re.sub('<div class=\"ad_content\".*?</div>','',context)\n",
        "  context=context.replace(\"<br/>\",\"\\n\").replace(\"</p>\",\"\\n\")\n",
        "  context=re.sub('<.*?>',\"\",context).split(\"\\n\")\n",
        "  context=[itm.strip() for itm in context if len(itm)>0]\n",
        "  text+=\"\".join(context)+\"\\n\\n\"\n",
        "\n",
        "  \n",
        "  \n",
        "  with open(f\"{art_id}.txt\",mode=\"w\",encoding=\"utf-8\") as f:\n",
        "    f.write(text)\n",
        "  \n",
        "\n",
        "#@markdown 書籍目錄網址\n",
        "url=\"https://tw.uukanshu.com/b/109951/\" #@param {type:'string'}\n",
        "sites=url[:url.find(\"/\",8)]\n",
        "\n",
        "reg=requests.get(url)\n",
        "soup=BeautifulSoup(reg.text,\"html.parser\")\n",
        "output_name=soup.find(\"h2\").getText()\n",
        "articles=soup.find(name=\"ul\",id=\"chapterList\").find_all(\"a\")\n",
        "\n",
        "links=[]\n",
        "# len(articles)\n",
        "for i in articles:\n",
        "  href=i.get(\"href\")\n",
        "  links.append([i.get(\"title\"),f\"{sites}{href}\"])\n",
        "links.sort(key=lambda x: x[1])\n",
        "\n",
        "for url in links[:4]:\n",
        "  get_html(url)\n",
        "\n",
        "\n",
        "# 同時建立及啟用10個執行緒\n",
        "with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
        "    executor.map(get_html, links)\n",
        "\n",
        "output_name=soup.find(\"h2\").getText()\n",
        "files_text=os.listdir()\n",
        "files_text=[file for file in files_text if file.endswith(\".txt\")]\n",
        "# 檔案排序，需要考慮 檔案名稱長短不一的問題，問前是透過數字的處理\n",
        "files_text.sort(key=lambda x:int(x[:-4]))\n",
        "with open(f\"../{output_name}.txt\",\"w\",encoding='utf-8') as f:\n",
        "  for file in files_text:\n",
        "    with open(file,\"r\") as f2:\n",
        "      f.write(f2.read())\n",
        "\n",
        "from google.colab import files\n",
        "files.download('../{}.txt'.format(output_name)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNxKvxlfhvO6"
      },
      "source": [
        "# 效率\n",
        "\n",
        "透過下述的方法，合併檔案，因為輸出檔需要被反覆的開始太多次，隨著檔案大小逐漸增加。讓效能下跌\n",
        "\n",
        "```python\n",
        "for file in files:\n",
        "  os.system(\"cat {}>> ../{}.txt\".format(file,output_name))\n",
        "```\n",
        "若改用下述的方法， output 檔，只需要開啟一次。可以大大縮短時間。\n",
        "\n",
        "```python\n",
        "with open(f\"../{output_name}.txt\",\"w\",encoding='utf-8') as f:\n",
        "  for file in files_text:\n",
        "    with open(file,\"r\") as f2:\n",
        "      f.write(f2.read())\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTCRsZSrork3",
        "cellView": "form"
      },
      "source": [
        "#@title 品書閣 專用(多執行緖)\n",
        "#@markdown 還在修正的程式，可以直接從這一個區塊執行\n",
        "\n",
        "import requests\n",
        "import os,re\n",
        "from bs4 import BeautifulSoup\n",
        "import concurrent.futures\n",
        "\n",
        "try:\n",
        "  os.mkdir(\"/content/tmp\")\n",
        "except:\n",
        "  print(\"目錄已存在\")\n",
        "os.chdir(\"/content/tmp\")\n",
        "os.system(\"rm -fr *\")\n",
        "\n",
        "stop_word=\"請大家收藏\" \n",
        "\n",
        "  \n",
        "def get_html(arr):\n",
        "  \n",
        "  [titles,links]=arr \n",
        "  art_id=links[links.rfind(\"/\")+1:-5]\n",
        "\n",
        "  soup=BeautifulSoup(requests.get(links).text)\n",
        "  content=soup.find(name=\"div\",id='content').find_all(\"p\")\n",
        "\n",
        "  text=f\"{titles}\\n\"\n",
        "\n",
        "  for itm in content[1:]:\n",
        "    txt=itm.getText()\n",
        "    if \"一秒記住\" in txt:\n",
        "      continue\n",
        "    if \"點下一章繼續閱讀\" in txt:\n",
        "      text+=f\"{txt}\\n\"\n",
        "      break\n",
        "    if stop_word in txt:\n",
        "      break    \n",
        "    # text+=\"%s \\n\" %txt\n",
        "    \n",
        "    text+=f\"{txt}\\n\"\n",
        "  \n",
        "  # print(art_id)\n",
        "  next_article=BeautifulSoup(requests.get(links).text).find(\"a\",text=\"下一章\").get(\"href\")\n",
        "  \n",
        "\n",
        "  if art_id in next_article:\n",
        "    \n",
        "    soup2=BeautifulSoup(requests.get(f\"{sites}{next_article}\").text)\n",
        "    content2=soup2.find(name=\"div\",id=\"content\").find_all(\"p\")\n",
        "    print(\"***\",next_article)\n",
        "    for itm2 in content2:\n",
        "      txt2=itm2.getText()\n",
        "      if \"一秒記住\" in txt2:\n",
        "        continue\n",
        "      if stop_word in txt2:\n",
        "        break          \n",
        "      text+=f\"{txt2}\\n\"\n",
        "   \n",
        "  with open(f\"{art_id}.txt\",mode=\"w\",encoding=\"utf-8\") as f:\n",
        "    f.write(text)\n",
        "\n",
        "\n",
        "#@markdown 書籍目錄網址\n",
        "url=\"https://tw.pinsuge.com/index/85767.html\" #@param {type:'string'}\n",
        "sites=url[:url.find(\"/\",8)]\n",
        "req = requests.get(url)\n",
        "soup=BeautifulSoup(req.text)\n",
        "articles=soup.find_all(\"a\")\n",
        "links=[]\n",
        "titles=[]\n",
        "for itm in articles[1:]:\n",
        "  if not itm.get(\"title\"):\n",
        "    continue \n",
        "  links.append([itm.get(\"title\",),\"{}{}\".format(sites,itm.get(\"href\"))])\n",
        "\n",
        "# 同時建立及啟用10個執行緒\n",
        "with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
        "    executor.map(get_html, links)\n",
        "\n",
        "files_text=os.listdir()\n",
        "files_text=[file for file in files_text if file.endswith(\".txt\")]\n",
        "# 檔案排序，需要考慮 檔案名稱長短不一的問題，問前是透過數字的處理\n",
        "files_text.sort(key=lambda x:int(x[:-4]))\n",
        "with open(f\"../{output_name}.txt\",\"w\",encoding='utf-8') as f:\n",
        "  for file in files_text:\n",
        "    with open(file,\"r\") as f2:\n",
        "      f.write(f2.read())\n",
        "      \n",
        "from google.colab import files\n",
        "files.download('../{}.txt'.format(output_name)) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KCQpgIUtZ3M"
      },
      "source": [
        "# 參考資料"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHTWOfq8CDZV",
        "cellView": "form"
      },
      "source": [
        "#@title BeauttifulSoup 練習\n",
        "#@markdown extract, decompose 的練習\n",
        "html_doc = \"\"\"<html><head><title>The Dormouse's story</title></head>\n",
        "<body>\n",
        "<p class=\"title\"><b>The Dormouse's story</b></p>\n",
        "\n",
        "\n",
        "<p class=\"story\">Once upon a time there were three little sisters; and their names were <script>.lkjlkjlj\\n\\r\n",
        "我是\\n</script> <i> hello world</i>\n",
        "<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>,\n",
        "<a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and\n",
        "<a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>;\n",
        "and they lived at the bottom of a well.</p>\n",
        "\n",
        "<p class=\"story\">...</p>\n",
        "\"\"\"\n",
        "from bs4 import BeautifulSoup\n",
        "soup=BeautifulSoup(html_doc,\"lxml\")\n",
        "txt=soup.find(\"p\",\"story\")\n",
        "print(str(txt))\n",
        "print(\"\".center(100,\"-\"))\n",
        "txt.script.decompose()\n",
        "\n",
        "txt.a.extract()\n",
        "txt.a.extract()\n",
        "\n",
        "print(str(txt))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PlVK__tP_Bi",
        "cellView": "form"
      },
      "source": [
        "#@title 品書閣 (舊) 單執行緖版本\n",
        "#@markdown 查看目錄=>章節列表\n",
        "import requests\n",
        "import os,re,time\n",
        "from bs4 import BeautifulSoup\n",
        "import concurrent.futures\n",
        "\n",
        "try:\n",
        "  os.mkdir(\"/content/tmp\")\n",
        "except:\n",
        "  print(\"目錄已存在\")\n",
        "os.chdir(\"/content/tmp\")\n",
        "os.system(\"rm -fr *\")\n",
        "\n",
        "url=\"https://tw.pinsuge.com/index/85767.html\" #@param {type:'string'}\n",
        "sites=url[:url.find(\"/\",8)]\n",
        "req = requests.get(url)\n",
        "#@markdown 在 stop_word 之後為相關的廣告內容\n",
        "stop_word=\"請大家收藏\" #@param {type:'string'}\n",
        "#@markdown 若章節分成兩個頁面的處理字元\n",
        "next_word=\"點下一章繼續\" #@param {type:'string'}\n",
        "soup=BeautifulSoup(req.text)\n",
        "\n",
        "soup\n",
        "articles=soup.find_all(name=\"a\")\n",
        "output_name=soup.find(\"h1\").getText()\n",
        "links=[]\n",
        "\n",
        "for itm in articles[1:]:\n",
        "  if not itm.get(\"title\"):\n",
        "    continue \n",
        "  links.append([itm.get(\"title\",),\"{}{}\".format(sites,itm.get(\"href\"))])\n",
        "\n",
        "\n",
        "def get_content(content):\n",
        "  to_text=''\n",
        "  for itm in content[:-1]:\n",
        "    txt=itm.getText()\n",
        "    if \"一秒記住\" in txt:\n",
        "      continue\n",
        "    if stop_word in txt:\n",
        "      break    \n",
        "    to_text+=\"%s \\n\" %txt\n",
        "  return to_text\n",
        "\n",
        "def get_soup(url):\n",
        "  return BeautifulSoup(requests.get(url).text)\n",
        "\n",
        "def cont_chapter_check(soup,text):\n",
        "  return (len(soup.find_all(name=\"p\",string=re.compile(text))) >0)\n",
        "\n",
        "def cont_chapter_link(soup):\n",
        "  return soup.find(\"a\",text=\"下一章\").get(\"href\")\n",
        "\n",
        "index=1\n",
        "star=time.time()\n",
        "for link in links:  \n",
        "  soup=get_soup(link[1])\n",
        "  to_text=''\n",
        "  to_text+=\"%s \\n\" %link[0]\n",
        "  print(link[0])\n",
        "  to_text+=get_content(soup.find(name=\"div\",id=\"content\").find_all(name=\"p\"))\n",
        "  \n",
        "  if cont_chapter_check(soup,next_word):\n",
        "    reg2=requests.get(\"{}{}\".format(sites,cont_chapter_link(soup)))\n",
        "    to_text+=get_content(BeautifulSoup(reg2.text).find(name=\"div\",id=\"content\").find_all(name=\"p\"))\n",
        "  # print(to_text)\n",
        "\n",
        "  with open(\"%03d.txt\" % index,mode=\"w\",encoding=\"utf-8\") as f:\n",
        "    f.write(to_text)\n",
        "  index+=1\n",
        "\n",
        "files=os.listdir()\n",
        "files=[file for file in files if file.endswith(\".txt\")]\n",
        "files.sort()\n",
        "files\n",
        "for file in files:\n",
        "  os.system(\"cat {}>> ../{}.txt\".format(file,output_name))\n",
        "\n",
        "from google.colab import files\n",
        "files.download('../{}.txt'.format(output_name)) \n",
        "end=time.time()\n",
        "print(\"經過了 {end-time} 秒\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CCMkYp5OxmZ",
        "cellView": "form"
      },
      "source": [
        "#@title 多執行序參考程式範例\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import concurrent.futures\n",
        "import requests\n",
        "import time\n",
        " \n",
        " \n",
        "def scrape(urls):\n",
        "  \n",
        "    response = requests.get(urls)\n",
        " \n",
        "    soup = BeautifulSoup(response.content, \"lxml\")\n",
        " \n",
        "    # 爬取文章標題\n",
        "    titles = soup.find_all(\"h3\", {\"class\": \"post_title\"})\n",
        " \n",
        "    for title in titles:\n",
        "        print(title.getText().strip())\n",
        " \n",
        "    time.sleep(2)\n",
        " \n",
        " \n",
        "base_url = \"https://www.inside.com.tw/tag/AI\"\n",
        "urls = [f\"{base_url}?page={page}\" for page in range(1, 6)]  # 1~5頁的網址清單\n",
        "print(urls) \n",
        "start_time = time.time()  # 開始時間\n",
        "# scrape(urls)\n",
        "\n",
        " \n",
        "# 同時建立及啟用10個執行緒\n",
        "# with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
        "#     executor.map(scrape, urls)\n",
        " \n",
        "end_time = time.time()\n",
        "print(f\"{end_time - start_time} 秒爬取 {len(urls)} 頁的文章\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}