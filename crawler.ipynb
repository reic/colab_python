{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/reic/colab_python/blob/main/crawler.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjvmI-ML-U4Y"
      },
      "source": [
        "# 網路爬蟲與多執行緒練習\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "wGkkH1MmUvuV",
        "outputId": "bb41e8bf-b851-4c24-e24a-7e165061705d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libcmark-gfm-extensions0.29.0.gfm.3 libcmark-gfm0.29.0.gfm.3 pandoc-data\n",
            "Suggested packages:\n",
            "  texlive-latex-recommended texlive-xetex texlive-luatex pandoc-citeproc texlive-latex-extra\n",
            "  context wkhtmltopdf librsvg2-bin groff ghc nodejs php python ruby libjs-mathjax libjs-katex\n",
            "  citation-style-language-styles\n",
            "The following NEW packages will be installed:\n",
            "  libcmark-gfm-extensions0.29.0.gfm.3 libcmark-gfm0.29.0.gfm.3 pandoc pandoc-data\n",
            "0 upgraded, 4 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 20.6 MB of archives.\n",
            "After this operation, 156 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libcmark-gfm0.29.0.gfm.3 amd64 0.29.0.gfm.3-3 [115 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libcmark-gfm-extensions0.29.0.gfm.3 amd64 0.29.0.gfm.3-3 [25.1 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 pandoc-data all 2.9.2.1-3ubuntu2 [81.8 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/universe amd64 pandoc amd64 2.9.2.1-3ubuntu2 [20.3 MB]\n",
            "Fetched 20.6 MB in 1s (33.0 MB/s)\n",
            "Selecting previously unselected package libcmark-gfm0.29.0.gfm.3:amd64.\n",
            "(Reading database ... 123634 files and directories currently installed.)\n",
            "Preparing to unpack .../libcmark-gfm0.29.0.gfm.3_0.29.0.gfm.3-3_amd64.deb ...\n",
            "Unpacking libcmark-gfm0.29.0.gfm.3:amd64 (0.29.0.gfm.3-3) ...\n",
            "Selecting previously unselected package libcmark-gfm-extensions0.29.0.gfm.3:amd64.\n",
            "Preparing to unpack .../libcmark-gfm-extensions0.29.0.gfm.3_0.29.0.gfm.3-3_amd64.deb ...\n",
            "Unpacking libcmark-gfm-extensions0.29.0.gfm.3:amd64 (0.29.0.gfm.3-3) ...\n",
            "Selecting previously unselected package pandoc-data.\n",
            "Preparing to unpack .../pandoc-data_2.9.2.1-3ubuntu2_all.deb ...\n",
            "Unpacking pandoc-data (2.9.2.1-3ubuntu2) ...\n",
            "Selecting previously unselected package pandoc.\n",
            "Preparing to unpack .../pandoc_2.9.2.1-3ubuntu2_amd64.deb ...\n",
            "Unpacking pandoc (2.9.2.1-3ubuntu2) ...\n",
            "Setting up libcmark-gfm0.29.0.gfm.3:amd64 (0.29.0.gfm.3-3) ...\n",
            "Setting up libcmark-gfm-extensions0.29.0.gfm.3:amd64 (0.29.0.gfm.3-3) ...\n",
            "Setting up pandoc-data (2.9.2.1-3ubuntu2) ...\n",
            "Setting up pandoc (2.9.2.1-3ubuntu2) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "Collecting inlp\n",
            "  Downloading iNLP-0.0.2-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting simhash (from inlp)\n",
            "  Downloading simhash-2.1.2-py3-none-any.whl.metadata (382 bytes)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from simhash->inlp) (1.26.4)\n",
            "Downloading iNLP-0.0.2-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading simhash-2.1.2-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: simhash, inlp\n",
            "Successfully installed inlp-0.0.2 simhash-2.1.2\n"
          ]
        }
      ],
      "source": [
        "#@title 必要元件安裝\n",
        "# def check_package(package_name,is_system_package=False):\n",
        "#   import importlib, subprocess, shutil\n",
        "#   if is_system_package:\n",
        "#     if(shutil.which(package_name)):\n",
        "#       print(f\"{package_name} 套件已經安裝\")\n",
        "#     else:\n",
        "#       print(f\"{package_name} 套件尚未安裝，正在安裝中...\")\n",
        "#       subprocess.check_call([\"apt-get\", \"install\", \"-y\", package_name])\n",
        "#   else:\n",
        "#     try:\n",
        "#       importlib.import_module(package_name)\n",
        "#       print(f\"{package_name} 套件已經安裝\")\n",
        "#     except ImportError:\n",
        "#       print(f\"{package_name} 套件尚未安裝，正在安裝中...\")\n",
        "#       subprocess.check_call([\"pip\", \"install\", package_name])\n",
        "\n",
        "# # 檢查 pandoc 是否已安裝，若無則安裝\n",
        "# check_package(\"pandoc\",is_system_package=True)\n",
        "# check_package(\"inlp\")\n",
        "\n",
        "!apt-get install -y pandoc\n",
        "!pip install inlp"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 可用網站清單\n",
        "\n",
        "* 小書包小說網：https://www.xiaoshubao.net/read/488175/1.html\n",
        "* 冬日小說網：https://www.drxsw.com/\n",
        "* UU看書：https://www.uuread.tw/\n"
      ],
      "metadata": {
        "id": "uPeuRtTlEhXp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hKEPfJ6aEPIj",
        "outputId": "ae640eeb-4548-40f7-bad3-82d28e978b03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['第1章 張揚大師兄', 'https://www.uuread.tw/chapter/1112129/9126.html'], ['第2章 我教你們修仙', 'https://www.uuread.tw/chapter/1112129/9128.html'], ['第3章 窮得可怕、富得流油', 'https://www.uuread.tw/chapter/1112129/9130.html'], ['第4章 飛劍劈柴', 'https://www.uuread.tw/chapter/1112129/9131.html']]\n",
            "['9126.html.txt', '9128.html.txt', '9130.html.txt', '9131.html.txt']\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import os,re\n",
        "from bs4 import BeautifulSoup\n",
        "import concurrent.futures\n",
        "from inlp.convert import chinese\n",
        "\n",
        "try:\n",
        "  os.mkdir(\"/content/tmp\")\n",
        "except:\n",
        "  print(\"目錄已存在\")\n",
        "os.chdir(\"/content/tmp\")\n",
        "os.system(\"rm -fr *\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#@markdown 書籍目錄網址\n",
        "url=\"https://www.uuread.tw/1112129\" #@param {type:'string'}\n",
        "title=\"誰教他這麼修仙的？\" #@param {type:\"string\"}\n",
        "author=\"可樂味令多情\" #@param {type:\"string\"}\n",
        "cssselector=\"#newlist > li:nth-child(n) > a\" #@param {type:\"string\"}\n",
        "#@markdown 打勾，將會直接變成 epub\n",
        "file2epub= True #@param {type:\"boolean\"}\n",
        "debugmode = True #@param {type:\"boolean\"}\n",
        "\n",
        "encode = \"utf-8\" # @param {\"type\":\"string\",\"placeholder\":\"utf-8\"}\n",
        "\n",
        "# 標題設定義\n",
        "YAML=f'''---\n",
        "title: {title}\n",
        "author: {author}\n",
        "language: zh-Hant\n",
        "---'''\n",
        "\n",
        "with open(\"title.txt\",mode=\"w\",encoding=\"utf-8\") as f:\n",
        "  f.write(YAML)\n",
        "\n",
        "sites=url[:url.find(\"/\",8)]\n",
        "#sites=url[:url.rfind(\"/\")]\n",
        "\n",
        "reg=requests.get(url)\n",
        "# soup=BeautifulSoup(reg.text,\"html.parser\")\n",
        "reg.encoding=encode\n",
        "soup=BeautifulSoup(reg.text)\n",
        "articles = soup.select(cssselector)\n",
        "# articles=soup.find(name=\"ul\",id=\"chapterList\").find_all(\"a\")\n",
        "\n",
        "links=[]\n",
        "# len(articles)\n",
        "for i in articles:\n",
        "  href=i.get(\"href\")\n",
        "  links.append([i.getText(),f\"{sites}{href}\"])\n",
        "\n",
        "files_text=[link[1][link[1].rfind(\"/\")+1:]+\".txt\" for link in links]\n",
        "if debugmode:\n",
        "  print(links[:4])\n",
        "  print(files_text[:4])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRS1C2TfF7QP",
        "outputId": "de4063bf-8a0b-4cd7-fa08-153892416730"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "187540.html\n",
            "# 第169章 大佬也打麻將？\n",
            "\n",
            "張揚不可思議地看著天花板，難不成那天花板上還藏著人？\n",
            "\n",
            "他再次小心翼翼地飛上天花板，這一次，他凝神去傾聽那些說話聲，看看能不能聽到一些什麼東西。\n",
            "\n",
            "可是，他發現依然還是聽不清。\n",
            "\n",
            "就好像在晚上做夢的時候，有人在夢中說了很多話，醒來卻什麼都記不住一樣。\n",
            "\n",
            "“是魔神低語嗎？”張揚眉頭皺了起來。\n",
            "\n",
            "他得到的一種魔族功法裡面，就有魔神低語，威力非常可怕。\n",
            "\n",
            "他看著黑漆漆、遙不可及的天花板，心中非常疑惑，這到底是怎麼回事？\n",
            "\n",
            "在連續試了幾次之後，張揚就放棄了。\n",
            "\n",
            "根本聽不清，也無法接近天花板，再試也沒有其他作用了。\n",
            "\n",
            "隨後，他開始尋找大殿的門，準備去尋找司徒明月他們。\n",
            "\n",
            "等他來到大門處的時候，卻發現如何也打不開。\n",
            "\n",
            "張揚用盡全力去推，大門依然紋絲不動，一點縫隙都沒有。\n",
            "\n",
            "“媽的，就給些功法也就罷了，還不給開門！”張揚惱怒地一個法術轟向那扇大門。\n",
            "\n",
            "大門依然紋絲不動，毫髮無損。\n",
            "\n",
            "“開門，給我開門！”\n",
            "\n",
            "張揚把他所會的各種法術，全部用來轟擊在大門上。反正大門不會抵抗，他全力凝聚靈力，一道足以轟死元嬰期的雷電，劈在那大門上，大門依然沒有任何反應。\n",
            "\n",
            "既然仙法不行，他乾脆就用剛剛學到的那些魔道功法。\n",
            "\n",
            "隨後，他全力調動金丹的力量，驅動魔道功法，然後，一道黑色的雷電猛然轟擊在大門上，大門緩緩地開啟了。\n",
            "\n",
            "“原來要用魔道功法才能開啟嗎？”\n",
            "\n",
            "張揚嘀咕著，走出了魔殿，才看清整個造化靈殿的格局，\n",
            "\n",
            "整個造化靈殿有點像四合院，周圍被四座同樣的大殿包圍，中間是一座小院。\n",
            "\n",
            "有花鋪、假山、水池，中心還有一張桌子，桌子周圍有四個座位，分別對應周圍的四座大殿。除了沒有看到出去的大門，這裡的一切都和四合院那麼像。\n",
            "\n",
            "張揚看著眼前的院子，有些發懵。\n",
            "\n",
            "在他看來，這裡好有生活氣息......說不定很久很久以前，仙、魔、妖、鬼四位大佬就生活在這裡。甚至閒著沒事的時候，說不定還聊天打麻將什麼的。\n",
            "\n",
            "尤其是當他看到那個桌子上，擺放著一些“籌碼”一樣的東西的時候，他更是確定了這種猜測。\n",
            "\n",
            "他跑到那桌子旁邊，仔細打量了一下那些“籌碼”，每一顆“籌碼”都圓滾滾的，像龍眼一樣大小，分成四色。對應魔殿大門的，乃是血黃色；在魔殿對面的，乃是明黃色；左面的，青黃色；右面的，褐黃色。\n",
            "\n",
            "四方的“籌碼”，數量各不相同。\n",
            "\n",
            "魔殿大門對應這一方的，有四十枚；魔殿大門對面那一方的，要多一些，五十五枚；左面青黃色的“籌碼”，明顯偏少，只有三十枚；右面的褐黃色“籌碼”是最多的，有足足七十枚！\n",
            "\n",
            "能夠被四個大佬用來當“籌碼”的......寶物！\n",
            "\n",
            "張揚意識到這些“籌碼”是寶物，立刻抓起那些“籌碼”，就往兜裡面揣\n",
            "\n",
            "他只顧著裝“籌碼”，卻沒發現那些“籌碼”在進入他的兜裡以後，全部化為四色的霧氣，進入了他的身體。\n",
            "\n",
            "張揚把所有“籌碼”一掃而空，發現兜裡重量沒有增加，他才開始疑惑起來。\n",
            "\n",
            "開啟一看，他看到那些還沒有消散的“籌碼”，正在化為霧氣，朝著他的身體裡面鑽進去。\n",
            "\n",
            "“臥槽，我身體是大雜燴嗎？什麼東西都往我身體裡面鑽？”\n",
            "\n",
            "\n",
            "187540.html\n",
            "他阻止不了，只能眼睜睜地看著四色霧氣，徹底消散在他的身體裡面。\n",
            "\n",
            "可是，當他仔細感應身體的時候，卻發現身體沒有任何變化。\n",
            "\n",
            "張揚的身體沒有任何變化，而此時，的天驕戰場，卻發生了巨大的震動。\n",
            "\n",
            "原本在天驕戰場之中，有幾條巨大的裂縫，把天驕戰場分為了四塊。\n",
            "\n",
            "可是，天驕戰場那些裂縫，就像是大地的傷口在癒合一樣，居然合攏了。\n",
            "\n",
            "整個天驕戰場，都在巨震。\n",
            "\n",
            "這種巨震，不但讓天驕戰場裡面的人都有明顯的感覺，甚至天驕戰場外的眾人，更是感應到天驕戰場的變化。\n",
            "\n",
            "眾多宗門的合道境，每個人都在眉頭緊皺，不知道天驕戰場發生了什麼。\n",
            "\n",
            "難道說，天驕戰場在三千年後，有了其他的變化？\n",
            "\n",
            "他們在猶豫，是不是要趕緊把這件事情，通知宗門祖師，讓那些渡劫境大能親自前來看看？\n",
            "\n",
            "不過天驕戰場的巨震，很快就平復了，眾多合道境，也就沒有把這個訊息傳遞回去。\n",
            "\n",
            "實際上，他們也在害怕。\n",
            "\n",
            "三千年前，各個大宗門有很多渡劫境就是死在了天驕戰場。\n",
            "\n",
            "這個地方是強者的墓地，反而是境界低的人安全。\n",
            "\n",
            "所以，能夠不讓祖師前來此處，他們覺得這是一種好現象。\n",
            "\n",
            "而此時的天驕戰場，各個據點的眾人，每個人都是神色凝重。\n",
            "\n",
            "這麼多年了，還從來沒有聽說過天驕戰場“地震”的。\n",
            "\n",
            "“查探一下，看看到底發生了什麼？”\n",
            "\n",
            "“天驕戰場不會破開了吧？我們可別忘記了，這可是一塊天外隕石啊！雖然太大了一些，但是，本質依然是天外隕石。”\n",
            "\n",
            "眾多大宗門的人，有些擔憂起來。\n",
            "\n",
            "很快，有人想到了天驕戰場的那幾條巨大的裂縫，立刻說道：“派人去那幾條裂縫看看，如果天驕戰場真的要裂開了，一定會從那幾條裂縫開始。\n",
            "\n",
            "另外，我們得趕緊準備一下，前往中心的大殿了。\n",
            "\n",
            "如果天驕戰場裂開，會不會把那座大殿撕開？\n",
            "\n",
            "要是把大殿撕開，我們就能夠進入那座大殿了。”\n",
            "\n",
            "“可是，我們準備得還不夠充分，時間距離以往的天驕戰場，也要早了兩個月。”\n",
            "\n",
            "“現在顧不得那麼多了。三千年都沒有開啟的大殿，現在很有可能開啟，我們得趕緊去看看。”\n",
            "\n",
            "所有大宗門的據點，大家都是天驕，都不是笨蛋，很快都想到了相同的問題。\n",
            "\n",
            "眾人留守一部分鎮守據點，另一部分人帶著準備的法寶、丹藥，前往中心的大殿了。\n",
            "\n",
            "而此時的大殿裡面，張揚在一頭霧水的情況下，把近兩百塊“籌碼”全部吸收了，還不知道外面發生了什麼事情。\n",
            "\n",
            "他檢視了一下桌面，就是一張普通的石桌，也沒有看到麻將這樣的玩意兒，不知道幾個大佬是怎麼賭“籌碼”的。\n",
            "\n",
            "然後，他才回頭去檢視那些假山、花圃、水池。\n",
            "\n",
            "當看清了花圃裡面的東西，張揚不由得揉了揉自已的眼睛，再次仔細觀看。\n",
            "\n",
            "張揚不由得尖叫起來：“天啊！”\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "content_selector=\"#nr > p:nth-child(n)\" #@param {type:\"string\"}\n",
        "#@markdown 打勾，將會直接變成 epub\n",
        "debug_content = True #@param {type:\"boolean\"}\n",
        "no_next_chapter = False # @param {\"type\":\"boolean\",\"placeholder\":\"True\"}\n",
        "next_page_selector = \"#nr_body > div:nth-child(2) > div > div > div > div.operate.text-center > a:nth-child(4)\" # @param {\"type\":\"string\"}\n",
        "find_All_p = False # @param {\"type\":\"boolean\",\"placeholder\":\"false\"}\n",
        "\n",
        "import lxml.html, time\n",
        "import re,os\n",
        "import json,ast\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_html(urls,pages=False):\n",
        "  time.sleep(2)\n",
        "  [title,art_url]=urls\n",
        "  art_id=art_url[art_url.rfind(\"/\")+1:]\n",
        "  if pages:\n",
        "    art_id=art_id[:art_id.rfind(\"_\")]+\".html\"\n",
        "  reg=requests.get(art_url)\n",
        "  reg.encoding=encode\n",
        "  soup=BeautifulSoup(reg.text)\n",
        "  # print(reg.text)\n",
        "  # content=soup.find(name=\"div\",class_='content')\n",
        "  content=soup.select(content_selector)\n",
        "  ## 我自己的修正\n",
        "  # if len(content)<1:\n",
        "  #   script_tags = soup.find_all(\"script\")\n",
        "  #   for script in script_tags:\n",
        "  #     if \"ChapterApp(\" in script.text:\n",
        "  #       match_content_info=re.search(r\"contentInfo:\\s*(\\[\\[.*?\\]\\]),\", script.text, re.DOTALL)\n",
        "  #       content_info_str = match_content_info.group(1)\n",
        "  #       content_info_str = content_info_str.replace(\"'\", \"\\\"\")\n",
        "  #       content_info = json.loads(content_info_str)[0]\n",
        "  #       print(content_info[0])\n",
        "\n",
        "\n",
        "  if find_All_p:\n",
        "    content=content[0].find_all(\"p\")\n",
        "  content=\"<br/>\".join([p.get_text() for p in content])\n",
        "\n",
        "  print(art_id)\n",
        "  textcon = f\"# {title}\\n\\n\" if not pages else \"\"\n",
        "  context=str(content).replace(\"\\n\",\"\").replace(\"\\r\",\"\")\n",
        "  context=context.replace(\"\\xa0\\xa0\\xa0\\xa0\",\"\")\n",
        "  context=re.sub('<div class=\"ad_content\".*?</div>','',context)\n",
        "  context=context.replace(\"<br/>\",\"\\n\").replace(\"</p>\",\"\\n\")\n",
        "  context=re.sub('<.*?>',\"\",context).split(\"\\n\")\n",
        "  context=[itm.strip() for itm in context if len(itm)>0]\n",
        "  textcon+=\"\\n\\n\".join(context)+\"\\n\\n\"\n",
        "  if debug_content:\n",
        "    print(chinese.s2t(textcon))\n",
        "\n",
        "  filemode=\"w\" if not pages else \"a\"\n",
        "  with open(f\"{art_id}.txt\",mode=filemode,encoding=\"utf-8\") as f:\n",
        "    f.write(chinese.s2t(textcon))\n",
        "  if no_next_chapter:\n",
        "    return\n",
        "  next_page = soup.select_one(next_page_selector)\n",
        "  if next_page and '_' in next_page['href']:\n",
        "    art_url = next_page['href'] if next_page['href'].startswith('https') else f\"{sites}{next_page['href']}\"\n",
        "    get_html([title,art_url],1)\n",
        "\n",
        "if debug_content:\n",
        "  get_html(links[0])\n",
        "else:\n",
        "  alreadydown=len(os.listdir())\n",
        "  if alreadydown>0:\n",
        "    links=links[alreadydown-1:]\n",
        "  for link in links:\n",
        "    get_html(link)\n",
        "\n",
        "    output_name=title\n",
        "    # files_text=os.listdir()\n",
        "    # files_text=[file for file in files_text if file.endswith(\".txt\")]\n",
        "    # 檔案排序，需要考慮 檔案名稱長短不一的問題，問前是透過數字的處理\n",
        "    # files_text.sort(key=lambda x:int(x[:-4]))\n",
        "  if file2epub:\n",
        "    os.system(\"pandoc -o \\\"../{}.epub\\\" title.txt {}\".format(title,\" \".join(files_text)))\n",
        "    from google.colab import files\n",
        "    files.download('../{}.epub'.format(title))\n",
        "    pass\n",
        "  else:\n",
        "    with open(f\"../{output_name}.txt\",\"w\",encoding='utf-8') as f:\n",
        "      for file in files_text:\n",
        "        with open(file,\"r\") as f2:\n",
        "          f.write(f2.read())\n",
        "    from google.colab import files\n",
        "    files.download('../{}.txt'.format(output_name))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "for filename in files_text:\n",
        "  with open(filename,\"r\") as f:\n",
        "    lines=f.readlines()\n",
        "  pattern = r\"^\\[?\\s*,?\\s*\"\n",
        "  new_lines = [re.sub(pattern, '', line) for line in lines if line.strip() != '' and line.strip()!=']']\n",
        "  with open(filename,\"w\",encoding='utf-8') as f:\n",
        "    f.write(\"\\n\".join(new_lines))\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KGwCkcRi1pjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.system(\"pandoc -o \\\"../{}.epub\\\" title.txt {}\".format(title,\" \".join(files_text)))\n",
        "from google.colab import files\n",
        "files.download('../{}.epub'.format(title))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "1MsnmgPURS10",
        "outputId": "50d7c5de-7a43-4782-c457-5fee3e2d7864"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_c798debc-3780-466c-a7ad-9181a3522f4a\", \"\\u8ab0\\u6559\\u4ed6\\u9019\\u9ebc\\u4fee\\u4ed9\\u7684\\uff1f.epub\", 2106344)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "files_text[0]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "VaBjYTELQiQ-",
        "outputId": "874b4e2f-78e1-46d1-adeb-97ad26d7dbb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'9126.html.txt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "vLrNyHB0M7LL"
      },
      "outputs": [],
      "source": [
        "#@title 小說狂人\n",
        "\n",
        "def check_package(package_name,is_system_package=False):\n",
        "  import importlib, subprocess, shutil\n",
        "  if is_system_package:\n",
        "    if(shutil.which(package_name)):\n",
        "      print(f\"{package_name} 套件已經安裝\")\n",
        "    else:\n",
        "      print(f\"{package_name} 套件尚未安裝，正在安裝中...\")\n",
        "      subprocess.check_call([\"apt-get\", \"install\", \"-y\", package_name])\n",
        "  else:\n",
        "    try:\n",
        "      importlib.import_module(package_name)\n",
        "      print(f\"{package_name} 套件已經安裝\")\n",
        "    except ImportError:\n",
        "      print(f\"{package_name} 套件尚未安裝，正在安裝中...\")\n",
        "      subprocess.check_call([\"pip\", \"install\", package_name])\n",
        "\n",
        "# 檢查 pandoc 是否已安裝，若無則安裝\n",
        "check_package(\"pandoc\",is_system_package=True)\n",
        "check_package(\"inlp\")\n",
        "\n",
        "\n",
        "import requests\n",
        "import os,re\n",
        "from bs4 import BeautifulSoup\n",
        "import concurrent.futures\n",
        "from inlp.convert import chinese\n",
        "\n",
        "try:\n",
        "  os.mkdir(\"/content/tmp\")\n",
        "except:\n",
        "  print(\"目錄已存在\")\n",
        "os.chdir(\"/content/tmp\")\n",
        "os.system(\"rm -fr *\")\n",
        "\n",
        "\n",
        "\n",
        "def get_html(urls):\n",
        "  [title,art_url]=urls\n",
        "  art_id=art_url[art_url.rfind(\"/\")+1:]\n",
        "  reg=requests.get(art_url)\n",
        "  reg.encoding=\"utf-8\"\n",
        "  soup=BeautifulSoup(reg.text)\n",
        "  content=soup.find(name=\"div\",class_='content')\n",
        "\n",
        "  print(art_id)\n",
        "  text=f\"# {title}\\n\\n\"\n",
        "  context=str(content).replace(\"\\n\",\"\").replace(\"\\r\",\"\")\n",
        "  context=context.replace(\"\\xa0\\xa0\\xa0\\xa0\",\"\")\n",
        "  context=re.sub('<div class=\"ad_content\".*?</div>','',context)\n",
        "  context=context.replace(\"<br/>\",\"\\n\").replace(\"</p>\",\"\\n\")\n",
        "  context=re.sub('<.*?>',\"\",context).split(\"\\n\")\n",
        "  context=[itm.strip() for itm in context if len(itm)>0]\n",
        "  text+=\"\\n\\n\".join(context)+\"\\n\\n\"\n",
        "\n",
        "  with open(f\"{art_id}.txt\",mode=\"w\",encoding=\"utf-8\") as f:\n",
        "    f.write(text)\n",
        "\n",
        "\n",
        "#@markdown 書籍目錄網址\n",
        "url=\"https://czbooks.net/n/s6g1n14e\" #@param {type:'string'}\n",
        "title=\"我在原始部落當酋長\" #@param {type:\"string\"}\n",
        "author=\"西原公子\" #@param {type:\"string\"}\n",
        "#@markdown 打勾，將會直接變成 epub\n",
        "file2epub = True #@param {type:\"boolean\"}\n",
        "\n",
        "# 標題設定義\n",
        "YAML=f'''---\n",
        "title: {title}\n",
        "author: {author}\n",
        "language: zh-Hant\n",
        "---'''\n",
        "\n",
        "with open(\"title.txt\",mode=\"w\",encoding='utf-8') as f:\n",
        "  f.write(YAML)\n",
        "\n",
        "sites=url[:url.find(\"/\",8)]\n",
        "# sites=url[:url.rfind(\"/\")]\n",
        "\n",
        "reg=requests.get(url)\n",
        "# soup=BeautifulSoup(reg.text,\"html.parser\")\n",
        "reg.encoding=\"utf-8\"\n",
        "soup=BeautifulSoup(reg.text)\n",
        "articles = soup.select('#chapter-list > li a')\n",
        "# articles=soup.find(name=\"ul\",id=\"chapterList\").find_all(\"a\")\n",
        "\n",
        "links=[]\n",
        "# len(articles)\n",
        "for i in articles:\n",
        "  href=i.get(\"href\")\n",
        "\n",
        "  links.append([i.text,f\"https:{href}\"])\n",
        "\n",
        "files_text=[link[1][link[1].rfind(\"/\")+1:]+\".txt\" for link in links]\n",
        "\n",
        "\n",
        "\n",
        "# # # 暫時無法使用，目前 colab 只要開放 2 個執行緖、只能運作 60 秒\n",
        "# # # # 同時建立及啟用10個執行緒\n",
        "# # # with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
        "# # #     executor.map(get_html, links)\n",
        "for link in links:\n",
        "  get_html(link)\n",
        "\n",
        "output_name=title\n",
        "# files_text=os.listdir()\n",
        "# files_text=[file for file in files_text if file.endswith(\".txt\")]\n",
        "# 檔案排序，需要考慮 檔案名稱長短不一的問題，問前是透過數字的處理\n",
        "# files_text.sort(key=lambda x:int(x[:-4]))\n",
        "if file2epub:\n",
        "  os.system(\"pandoc -o \\\"../{}.epub\\\" title.txt {}\".format(title,\" \".join(files_text)))\n",
        "  from google.colab import files\n",
        "  files.download('../{}.epub'.format(title))\n",
        "  pass\n",
        "else:\n",
        "  with open(f\"../{output_name}.txt\",\"w\",encoding='utf-8') as f:\n",
        "    for file in files_text:\n",
        "      with open(file,\"r\") as f2:\n",
        "        f.write(f2.read())\n",
        "  from google.colab import files\n",
        "  files.download('../{}.txt'.format(output_name))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "yONyl870ergz"
      },
      "outputs": [],
      "source": [
        "#@title UU看書網\n",
        "#@markdown 還在修正的程式，可以直接從這一個區塊執行\n",
        "\n",
        "def check_package(itm):\n",
        "  import importlib\n",
        "  try:\n",
        "    importlib.import_module(itm)\n",
        "    print(f\"{itm} 套件已經安裝\")\n",
        "  except ImportError:\n",
        "    print(f\"{itm} 套件尚未安裝，正在安裝中...\")\n",
        "    !pip install {itm}\n",
        "\n",
        "\n",
        "check_package(\"inlp\")\n",
        "\n",
        "import requests\n",
        "import os,re\n",
        "from bs4 import BeautifulSoup\n",
        "import concurrent.futures\n",
        "from inlp.convert import chinese\n",
        "\n",
        "try:\n",
        "  os.mkdir(\"/content/tmp\")\n",
        "except:\n",
        "  print(\"目錄已存在\")\n",
        "os.chdir(\"/content/tmp\")\n",
        "os.system(\"rm -fr *\")\n",
        "\n",
        "\n",
        "def get_html(urls):\n",
        "  [title,art_url]=urls\n",
        "  art_id=art_url[art_url.rfind(\"/\")+1:-5]\n",
        "  reg=requests.get(art_url)\n",
        "  reg.encoding=\"utf-8\"\n",
        "  soup=BeautifulSoup(reg.text)\n",
        "  content=soup.find(name=\"div\",id='contentbox')\n",
        "  print(art_id)\n",
        "  text=f\"# {title}\\n\\n\"\n",
        "  context=str(content).replace(\"\\n\",\"\").replace(\"\\r\",\"\")\n",
        "  context=context.replace(\"\\xa0\\xa0\\xa0\\xa0\",\"\")\n",
        "  context=re.sub('<div class=\"ad_content\".*?</div>','',context)\n",
        "  context=context.replace(\"<br/>\",\"\\n\").replace(\"</p>\",\"\\n\")\n",
        "  context=re.sub('<.*?>',\"\",context).split(\"\\n\")\n",
        "  context=[itm.strip() for itm in context if len(itm)>0]\n",
        "  text+=\"\\n\\n\".join(context)+\"\\n\\n\"\n",
        "\n",
        "  with open(f\"{art_id}.txt\",mode=\"w\",encoding=\"utf-8\") as f:\n",
        "    f.write(text)\n",
        "\n",
        "\n",
        "#@markdown 書籍目錄網址\n",
        "url=\"https://tw.uukanshu.com/b/239755/\" #@param {type:'string'}\n",
        "title=\"\\u6211\\u5BB6\\u5A18\\u5B50\\uFF0C\\u4E0D\\u5C0D\\u52C1\" #@param {type:\"string\"}\n",
        "author=\"\\u4E00\\u87EC\\u77E5\\u590F\" #@param {type:\"string\"}\n",
        "#@markdown 打勾，將會直接變成 epub\n",
        "file2epub = True #@param {type:\"boolean\"}\n",
        "\n",
        "# 標題設定義\n",
        "YAML=f'''---\n",
        "title: {title}\n",
        "author: {author}\n",
        "language: zh-Hant\n",
        "---'''\n",
        "\n",
        "with open(\"title.txt\",mode=\"w\",encoding='utf-8') as f:\n",
        "  f.write(YAML)\n",
        "\n",
        "sites=url[:url.find(\"/\",8)]\n",
        "# sites=url[:url.rfind(\"/\")]\n",
        "\n",
        "reg=requests.get(url)\n",
        "# soup=BeautifulSoup(reg.text,\"html.parser\")\n",
        "reg.encoding=\"utf-8\"\n",
        "soup=BeautifulSoup(reg.text)\n",
        "articles=soup.find(name=\"ul\",id=\"chapterList\").find_all(\"a\")\n",
        "\n",
        "links=[]\n",
        "# len(articles)\n",
        "for i in articles:\n",
        "  href=i.get(\"href\")\n",
        "  links.append([i.get(\"title\"),f\"{sites}{href}\"])\n",
        "# links.sort(key=lambda x: x[1])\n",
        "files_text=[link[1][link[1].rfind(\"/\")+1:link[1].rfind(\".\")]+\".txt\" for link in links]\n",
        "\n",
        "# 暫時無法使用，目前 colab 只要開放 2 個執行緖、只能運作 60 秒\n",
        "# # 同時建立及啟用10個執行緒\n",
        "# with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
        "#     executor.map(get_html, links)\n",
        "for link in links:\n",
        "  get_html(link)\n",
        "\n",
        "output_name=title\n",
        "# files_text=os.listdir()\n",
        "# files_text=[file for file in files_text if file.endswith(\".txt\")]\n",
        "# 檔案排序，需要考慮 檔案名稱長短不一的問題，問前是透過數字的處理\n",
        "# files_text.sort(key=lambda x:int(x[:-4]))\n",
        "if file2epub:\n",
        "  mdfiles=[ itm for itm in files_text[::-1] ]\n",
        "  os.system(\"pandoc -o \\\"../{}.epub\\\" title.txt {}\".format(title,\" \".join(mdfiles)))\n",
        "  from google.colab import files\n",
        "  files.download('../{}.epub'.format(title))\n",
        "  pass\n",
        "else:\n",
        "  with open(f\"../{output_name}.txt\",\"w\",encoding='utf-8') as f:\n",
        "    for file in files_text[::-1]:\n",
        "      with open(file,\"r\") as f2:\n",
        "        f.write(f2.read())\n",
        "  from google.colab import files\n",
        "  files.download('../{}.txt'.format(output_name))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNaQIMaFVWba",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title UU看書網\n",
        "#@markdown 還在修正的程式，可以直接從這一個區塊執行\n",
        "\n",
        "def check_package(itm):\n",
        "  import importlib\n",
        "  try:\n",
        "    importlib.import_module(itm)\n",
        "    print(f\"{itm} 套件已經安裝\")\n",
        "  except ImportError:\n",
        "    print(f\"{itm} 套件尚未安裝，正在安裝中...\")\n",
        "    !pip install {itm}\n",
        "\n",
        "\n",
        "check_package(\"inlp\")\n",
        "\n",
        "import requests\n",
        "import os,re\n",
        "from bs4 import BeautifulSoup\n",
        "import concurrent.futures\n",
        "from inlp.convert import chinese\n",
        "\n",
        "try:\n",
        "  os.mkdir(\"/content/tmp\")\n",
        "  chk_cont=False\n",
        "except:\n",
        "  print(\"目錄已存在\")\n",
        "  chk_cont=True\n",
        "os.chdir(\"/content/tmp\")\n",
        "# os.system(\"rm -fr *\")\n",
        "\n",
        "\n",
        "def get_html(urls):\n",
        "  [title,art_url]=urls\n",
        "  art_id=art_url[art_url.rfind(\"/\")+1:-5]\n",
        "  reg=requests.get(art_url)\n",
        "  reg.encoding=\"utf-8\"\n",
        "  soup=BeautifulSoup(reg.text)\n",
        "  # content=\"<br>\\n\".join([tag.string for tag in soup.find(name=\"div\",id='nr').find_all(\"p\")])\n",
        "  tmp=soup.find(name=\"div\",id='nr').find_all(\"p\")\n",
        "  print(art_id)\n",
        "  # pages=soup.find('h1').getText()\n",
        "  # numbers = re.findall(r'\\d+', pages)\n",
        "  check=soup.find(\"div\",class_=\"operate\").find_all(\"a\")\n",
        "  if (check[-1].getText()==\"下一頁\"):\n",
        "    page_url=sites+check[-1].get(\"href\")\n",
        "  # if (len(numbers) >1):\n",
        "  #   last_number=numbers[-1]\n",
        "  #   # print(last_number)\n",
        "  #   for i in range(2,int(last_number)+1):\n",
        "  #     page_url=art_url[:-5]+f\"_{i}.html\"\n",
        "    content_extend=get_html_page(page_url)\n",
        "    tmp=tmp+content_extend\n",
        "\n",
        "\n",
        "  content=\"\\n\".join([str(itm) for itm in tmp])\n",
        "  # print(\"<br>\".join([tag.string for tag in content]))\n",
        "  text=f\"# {title}\\n\\n\"\n",
        "  context=str(content).replace(\"\\n\",\"\").replace(\"\\r\",\"\")\n",
        "  context=context.replace(\"\\xa0\\xa0\\xa0\\xa0\",\"\")\n",
        "  context=re.sub('<div class=\"ad_content\".*?</div>','',context)\n",
        "  context=context.replace(\"<br/>\",\"\\n\").replace(\"</p>\",\"\\n\")\n",
        "  context=re.sub('<.*?>',\"\",context).split(\"\\n\")\n",
        "  context=[itm.strip() for itm in context if len(itm)>0]\n",
        "  text+=\"\\n\\n\".join(context)+\"\\n\\n\"\n",
        "\n",
        "  with open(f\"{art_id}.txt\",mode=\"w\",encoding=\"utf-8\") as f:\n",
        "    f.write(text)\n",
        "\n",
        "\n",
        "def get_html_page(page_url):\n",
        "  reg=requests.get(page_url)\n",
        "\n",
        "  reg.encoding=\"utf-8\"\n",
        "  soup=BeautifulSoup(reg.text)\n",
        "  tmp1=soup.find(name=\"div\",id='nr').find_all(\"p\")\n",
        "  check=soup.find(\"div\",class_=\"operate\").find_all(\"a\")\n",
        "  if (check[-1].getText()==\"下一頁\"):\n",
        "    page_url=sites+check[-1].get(\"href\")\n",
        "  # if (len(numbers) >1):\n",
        "  #   last_number=numbers[-1]\n",
        "  #   # print(last_number)\n",
        "  #   for i in range(2,int(last_number)+1):\n",
        "  #     page_url=art_url[:-5]+f\"_{i}.html\"\n",
        "    content_extend=get_html_page(page_url)\n",
        "    tmp1=tmp1+content_extend\n",
        "\n",
        "  return tmp1\n",
        "\n",
        "\n",
        "\n",
        "#@markdown 書籍目錄網址\n",
        "url=\"https://www.uuread.tw/28100\" #@param {type:'string'}\n",
        "title=\"系統賦我長生，我熬死了所有人\" #@param {type:\"string\"}\n",
        "author=\"一隻榴蓮3號\" #@param {type:\"string\"}\n",
        "#@markdown 打勾，將會直接變成 epub\n",
        "file2epub = True #@param {type:\"boolean\"}\n",
        "\n",
        "# 標題設定義\n",
        "YAML=f'''---\n",
        "title: {title}\n",
        "author: {author}\n",
        "language: zh-Hant\n",
        "---'''\n",
        "\n",
        "with open(\"title.txt\",mode=\"w\",encoding='utf-8') as f:\n",
        "  f.write(YAML)\n",
        "\n",
        "sites=url[:url.find(\"/\",8)]\n",
        "# sites=url[:url.rfind(\"/\")]\n",
        "\n",
        "reg=requests.get(url)\n",
        "# soup=BeautifulSoup(reg.text,\"html.parser\")\n",
        "reg.encoding=\"utf-8\"\n",
        "soup=BeautifulSoup(reg.text)\n",
        "articles=soup.find(name=\"ul\",id=\"newlist\").find_all(\"a\")\n",
        "\n",
        "links=[]\n",
        "# len(articles)\n",
        "for i in articles:\n",
        "  href=i.get(\"href\")\n",
        "  links.append([i.get(\"title\"),f\"{sites}{href}\"])\n",
        "# links.sort(key=lambda x: x[1])\n",
        "files_text=[link[1][link[1].rfind(\"/\")+1:link[1].rfind(\".\")]+\".txt\" for link in links]\n",
        "\n",
        "# # 暫時無法使用，目前 colab 只要開放 2 個執行緖、只能運作 60 秒\n",
        "# # # 同時建立及啟用10個執行緒\n",
        "# with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
        "#     executor.map(get_html, links)\n",
        "\n",
        "if chk_cont:\n",
        "  start_num=int(len(os.listdir())-1)\n",
        "  print(start_num)\n",
        "else:\n",
        "  start_num=0\n",
        "for index,link in enumerate(links[start_num:],start_num+1):\n",
        "  print(index)\n",
        "  get_html(link)\n",
        "\n",
        "output_name=title\n",
        "# files_text=os.listdir()\n",
        "# files_text=[file for file in files_text if file.endswith(\".txt\")]\n",
        "# 檔案排序，需要考慮 檔案名稱長短不一的問題，問前是透過數字的處理\n",
        "# files_text.sort(key=lambda x:int(x[:-4]))\n",
        "if file2epub:\n",
        "  mdfiles=[ itm for itm in files_text ]\n",
        "  os.system(\"pandoc -o \\\"../{}.epub\\\" title.txt {}\".format(title,\" \".join(mdfiles)))\n",
        "  from google.colab import files\n",
        "  files.download('../{}.epub'.format(title))\n",
        "  pass\n",
        "else:\n",
        "  with open(f\"../{output_name}.txt\",\"w\",encoding='utf-8') as f:\n",
        "    for file in files_text[::-1]:\n",
        "      with open(file,\"r\") as f2:\n",
        "        f.write(f2.read())\n",
        "  from google.colab import files\n",
        "  files.download('../{}.txt'.format(output_name))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "stXbsG0Qb0wC"
      },
      "outputs": [],
      "source": [
        "#@title UU看書網(修正版)\n",
        "#@markdown 還在修正的程式，可以直接從這一個區塊執行\n",
        "\n",
        "def check_package(package_name,is_system_package=False):\n",
        "  import importlib\n",
        "  import subprocess\n",
        "  try:\n",
        "    importlib.import_module(package_name)\n",
        "    print(f\"{package_name} 套件已經安裝\")\n",
        "  except ImportError:\n",
        "    print(f\"{package_name} 套件尚未安裝，正在安裝中...\")\n",
        "    if is_system_package:\n",
        "      subprocess.check_call([\"apt-get\", \"install\", \"-y\", package_name])\n",
        "    else:\n",
        "      subprocess.check_call([\"pip\", \"install\", package_name])\n",
        "\n",
        "# 檢查 pandoc 是否已安裝，若無則安裝\n",
        "check_package(\"pandoc\")\n",
        "\n",
        "check_package(\"inlp\")\n",
        "\n",
        "import requests\n",
        "import os,re\n",
        "from bs4 import BeautifulSoup\n",
        "import concurrent.futures\n",
        "from inlp.convert import chinese\n",
        "import time\n",
        "import random\n",
        "\n",
        "\n",
        "\n",
        "try:\n",
        "  os.mkdir(\"/content/tmp\")\n",
        "  chk_cont=False\n",
        "except:\n",
        "  print(\"目錄已存在\")\n",
        "  chk_cont=True\n",
        "os.chdir(\"/content/tmp\")\n",
        "# os.system(\"rm -fr *\")\n",
        "\n",
        "def get_article_content(article_url):\n",
        "    try:\n",
        "        response = requests.get(article_url,headers=headers, timeout=10)\n",
        "        response.encoding = \"utf-8\"\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "            content_div = soup.find('div', id='nr')\n",
        "            if content_div:\n",
        "                paragraphs = content_div.find_all('p')\n",
        "                content = '\\n'.join([p.get_text(strip=True) for p in paragraphs])\n",
        "\n",
        "                next_button_text = soup.find(\"div\", class_=\"operate\").find_all(\"a\")[-1].getText()\n",
        "                next_page_url = None\n",
        "                if next_button_text == \"下一頁\":\n",
        "                    next_page_url = sites + soup.find(\"div\", class_=\"operate\").find_all(\"a\")[-1].get(\"href\")\n",
        "\n",
        "                return content, next_page_url\n",
        "            else:\n",
        "                print(f\"無法找到內容：{article_url}\")\n",
        "        else:\n",
        "            print(f\"無法訪問網頁：{article_url}\")\n",
        "    except Exception as e:\n",
        "        print(f\"發生錯誤：{e}\")\n",
        "    return None, None\n",
        "\n",
        "def save_to_text_file(title, content, file_name, add_title=True):\n",
        "    try:\n",
        "        with open(file_name, 'a', encoding='utf-8') as file:\n",
        "            if add_title:\n",
        "                file.write(f\"# {title}\\n\\n\")\n",
        "            # 移除多餘的空白和換行\n",
        "            # print(content)\n",
        "            content = BeautifulSoup(content, \"html.parser\").get_text(separator=\"\\n\\n\")\n",
        "            content = content.replace(\"\\xa0\\xa0\\xa0\\xa0\", \"\").replace(\"<br/>\", \"\\n\").replace(\"</p>\", \"\\n\")\n",
        "\n",
        "            file.write(str(content))\n",
        "            print(f\"已保存至文件：{file_name}\")\n",
        "    except Exception as e:\n",
        "        print(f\"保存文件時出錯：{e}\")\n",
        "\n",
        "def crawl_articles(urls,start_num):\n",
        "\n",
        "    for index, (title, article_url) in enumerate(urls, start=start_num):\n",
        "        article_id = article_url.split('/')[-1].split('.')[0]\n",
        "        print(f\"正在處理文章：{article_id}\")\n",
        "        content, next_page_url = get_article_content(article_url)\n",
        "        if content:\n",
        "            # 保存第一頁的內容\n",
        "            file_name = f\"{article_id}.txt\"\n",
        "            save_to_text_file(title, content, file_name)\n",
        "\n",
        "            # 接著處理其他頁面（如果有）\n",
        "            while next_page_url:\n",
        "                next_page_content, next_page_url = get_article_content(next_page_url)\n",
        "                if next_page_content:\n",
        "                    # 保存其他頁面的內容\n",
        "                    save_to_text_file(title, next_page_content, file_name, add_title=False)\n",
        "\n",
        "\n",
        "def get_html(urls):\n",
        "  [title,art_url]=urls\n",
        "  art_id=art_url[art_url.rfind(\"/\")+1:-5]\n",
        "  reg=requests.get(art_url,headers=headers, timeout=10)\n",
        "  reg.encoding=\"utf-8\"\n",
        "  soup=BeautifulSoup(reg.text)\n",
        "  # content=\"<br>\\n\".join([tag.string for tag in soup.find(name=\"div\",id='nr').find_all(\"p\")])\n",
        "  tmp=soup.find(name=\"div\",id='nr').find_all(\"p\")\n",
        "  print(art_id)\n",
        "  # pages=soup.find('h1').getText()\n",
        "  # numbers = re.findall(r'\\d+', pages)\n",
        "  check=soup.find(\"div\",class_=\"operate\").find_all(\"a\")\n",
        "  if (check[-1].getText()==\"下一頁\"):\n",
        "    page_url=sites+check[-1].get(\"href\")\n",
        "  # if (len(numbers) >1):\n",
        "  #   last_number=numbers[-1]\n",
        "  #   # print(last_number)\n",
        "  #   for i in range(2,int(last_number)+1):\n",
        "  #     page_url=art_url[:-5]+f\"_{i}.html\"\n",
        "    content_extend=get_html_page(page_url)\n",
        "    tmp=tmp+content_extend\n",
        "\n",
        "\n",
        "  content=\"\\n\".join([str(itm) for itm in tmp])\n",
        "  # print(\"<br>\".join([tag.string for tag in content]))\n",
        "  text=f\"# {title}\\n\\n\"\n",
        "  context=str(content).replace(\"\\n\",\"\").replace(\"\\r\",\"\")\n",
        "  context=context.replace(\"\\xa0\\xa0\\xa0\\xa0\",\"\")\n",
        "  context=re.sub('<div class=\"ad_content\".*?</div>','',context)\n",
        "  context=context.replace(\"<br/>\",\"\\n\").replace(\"</p>\",\"\\n\")\n",
        "  context=re.sub('<.*?>',\"\",context).split(\"\\n\")\n",
        "  context=[itm.strip() for itm in context if len(itm)>0]\n",
        "  text+=\"\\n\\n\".join(context)+\"\\n\\n\"\n",
        "\n",
        "  with open(f\"{art_id}.txt\",mode=\"w\",encoding=\"utf-8\") as f:\n",
        "    f.write(text)\n",
        "\n",
        "\n",
        "def get_html_page(page_url):\n",
        "  reg=requests.get(page_url,headers=headers, timeout=10)\n",
        "\n",
        "  reg.encoding=\"utf-8\"\n",
        "  soup=BeautifulSoup(reg.text)\n",
        "  tmp1=soup.find(name=\"div\",id='nr').find_all(\"p\")\n",
        "  check=soup.find(\"div\",class_=\"operate\").find_all(\"a\")\n",
        "  if (check[-1].getText()==\"下一頁\"):\n",
        "    page_url=sites+check[-1].get(\"href\")\n",
        "  # if (len(numbers) >1):\n",
        "  #   last_number=numbers[-1]\n",
        "  #   # print(last_number)\n",
        "  #   for i in range(2,int(last_number)+1):\n",
        "  #     page_url=art_url[:-5]+f\"_{i}.html\"\n",
        "    content_extend=get_html_page(page_url)\n",
        "    tmp1=tmp1+content_extend\n",
        "\n",
        "  return tmp1\n",
        "\n",
        "\n",
        "\n",
        "#@markdown 書籍目錄網址\n",
        "url=\"https://www.uuread.tw/94294\" #@param {type:'string'}\n",
        "title=\"阿斗之智近乎妖\" #@param {type:\"string\"}\n",
        "author=\"漢衛\" #@param {type:\"string\"}\n",
        "#@markdown 打勾，將會直接變成 epub\n",
        "file2epub = True #@param {type:\"boolean\"}\n",
        "\n",
        "# 標題設定義\n",
        "YAML=f'''---\n",
        "title: {title}\n",
        "author: {author}\n",
        "language: zh-Hant\n",
        "---'''\n",
        "\n",
        "with open(\"title.txt\",mode=\"w\",encoding='utf-8') as f:\n",
        "  f.write(YAML)\n",
        "\n",
        "sites=url[:url.find(\"/\",8)]\n",
        "# sites=url[:url.rfind(\"/\")]\n",
        "\n",
        "reg=requests.get(url)\n",
        "# soup=BeautifulSoup(reg.text,\"html.parser\")\n",
        "reg.encoding=\"utf-8\"\n",
        "soup=BeautifulSoup(reg.text)\n",
        "articles=soup.find(name=\"ul\",id=\"newlist\").find_all(\"a\")\n",
        "\n",
        "links=[]\n",
        "# len(articles)\n",
        "for i in articles:\n",
        "  href=i.get(\"href\")\n",
        "  links.append([i.get(\"title\"),f\"{sites}{href}\"])\n",
        "# links.sort(key=lambda x: x[1])\n",
        "files_text=[link[1][link[1].rfind(\"/\")+1:link[1].rfind(\".\")]+\".txt\" for link in links]\n",
        "\n",
        "# # 暫時無法使用，目前 colab 只要開放 2 個執行緖、只能運作 60 秒\n",
        "# # # 同時建立及啟用10個執行緒\n",
        "# with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
        "#     executor.map(get_html, links)\n",
        "\n",
        "if chk_cont:\n",
        "  start_num=int(len(os.listdir())-1)\n",
        "  print(start_num)\n",
        "else:\n",
        "  start_num=0\n",
        "for index,link in enumerate(links[start_num:],start_num+1):\n",
        "  print(index)\n",
        "  get_html(link)\n",
        "  time.sleep(random.uniform(1, 2))\n",
        "\n",
        "# start_getnum=len(os.listdir(\".\"))-1\n",
        "\n",
        "# crawl_articles(links[start_getnum:],start_num)\n",
        "\n",
        "\n",
        "\n",
        "output_name=title\n",
        "# # files_text=os.listdir()\n",
        "# # files_text=[file for file in files_text if file.endswith(\".txt\")]\n",
        "# # 檔案排序，需要考慮 檔案名稱長短不一的問題，問前是透過數字的處理\n",
        "# # files_text.sort(key=lambda x:int(x[:-4]))\n",
        "\n",
        "!apt-get install -y\n",
        "if file2epub:\n",
        "  mdfiles=[ itm for itm in files_text ]\n",
        "  os.system(\"pandoc -o \\\"../{}.epub\\\" title.txt {}\".format(title,\" \".join(mdfiles)))\n",
        "  from google.colab import files\n",
        "  files.download('../{}.epub'.format(title))\n",
        "  pass\n",
        "else:\n",
        "  with open(f\"../{output_name}.txt\",\"w\",encoding='utf-8') as f:\n",
        "    for file in files_text[::-1]:\n",
        "      with open(file,\"r\") as f2:\n",
        "        f.write(f2.read())\n",
        "  from google.colab import files\n",
        "  files.download('../{}.txt'.format(output_name))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "e21JYlPGnuy7"
      },
      "outputs": [],
      "source": [
        "#@title UU看書\n",
        "#@markdown 還在修正的程式，可以直接從這一個區塊執行\n",
        "\n",
        "def check_package(itm):\n",
        "  import importlib\n",
        "  try:\n",
        "    importlib.import_module(itm)\n",
        "    print(f\"{itm} 套件已經安裝\")\n",
        "  except ImportError:\n",
        "    print(f\"{itm} 套件尚未安裝，正在安裝中...\")\n",
        "    !pip install {itm}\n",
        "\n",
        "\n",
        "check_package(\"inlp\")\n",
        "\n",
        "import requests\n",
        "import os,re\n",
        "from bs4 import BeautifulSoup\n",
        "import concurrent.futures\n",
        "from inlp.convert import chinese\n",
        "\n",
        "try:\n",
        "  os.mkdir(\"/content/tmp\")\n",
        "except:\n",
        "  print(\"目錄已存在\")\n",
        "os.chdir(\"/content/tmp\")\n",
        "os.system(\"rm -fr *\")\n",
        "\n",
        "\n",
        "def get_html(urls):\n",
        "  [title,art_url]=urls\n",
        "  art_id=art_url[art_url.rfind(\"/\")+1:-5]\n",
        "  reg=requests.get(art_url)\n",
        "  reg.encoding=\"utf-8\"\n",
        "  soup=BeautifulSoup(reg.text)\n",
        "  content=soup.find(name=\"div\",id='TextContent')\n",
        "  print(art_id)\n",
        "  text=f\"# {title}\\n\\n\"\n",
        "  context=str(content).replace(\"\\n\",\"\").replace(\"\\r\",\"\")\n",
        "  context=context.replace(\"\\xa0\\xa0\\xa0\\xa0\",\"\")\n",
        "  context=re.sub('<div class=\"ad_content\".*?</div>','',context)\n",
        "  context=context.replace(\"<br/>\",\"\\n\").replace(\"</p>\",\"\\n\")\n",
        "  context=re.sub('<.*?>',\"\",context).split(\"\\n\")\n",
        "  context=[itm.strip() for itm in context if len(itm)>0]\n",
        "  text+=\"\\n\\n\".join(context)+\"\\n\\n\"\n",
        "  text=chinese.s2t(text)\n",
        "\n",
        "  with open(f\"{art_id}.txt\",mode=\"w\",encoding=\"utf-8\") as f:\n",
        "    f.write(text)\n",
        "\n",
        "\n",
        "#@markdown 書籍目錄網址\n",
        "url=\"https://www.uuks5.com/book/632203/\" #@param {type:'string'}\n",
        "title=\"我在古代當便宜爹\" #@param {type:\"string\"}\n",
        "author=\"雲山風海\" #@param {type:\"string\"}\n",
        "#@markdown 打勾，將會直接變成 epub\n",
        "file2epub = True #@param {type:\"boolean\"}\n",
        "\n",
        "# 標題設定義\n",
        "YAML=f'''---\n",
        "title: {title}\n",
        "author: {author}\n",
        "language: zh-Hant\n",
        "---'''\n",
        "\n",
        "with open(\"title.txt\",mode=\"w\",encoding='utf-8') as f:\n",
        "  f.write(YAML)\n",
        "\n",
        "sites=url[:url.find(\"/\",8)]\n",
        "# sites=url[:url.rfind(\"/\")]\n",
        "\n",
        "reg=requests.get(url)\n",
        "soup=BeautifulSoup(reg.text,\"html.parser\")\n",
        "reg.encoding=\"utf-8\"\n",
        "\n",
        "\n",
        "articles=soup.find(name=\"ul\",id=\"chapterList\").find_all(name=\"a\")\n",
        "\n",
        "links=[]\n",
        "# # len(articles)\n",
        "for i in articles:\n",
        "  href=i.get(\"href\")\n",
        "  # print(i.text)\n",
        "  links.append([i.text,f\"{sites}{href}\"])\n",
        "# links.sort(key=lambda x: x[1])\n",
        "files_text=[link[1][link[1].rfind(\"/\")+1:link[1].rfind(\".\")]+\".txt\" for link in links]\n",
        "\n",
        "\n",
        "for link in links:\n",
        "  get_html(link)\n",
        "\n",
        "output_name=title\n",
        "# files_text=os.listdir()\n",
        "# files_text=[file for file in files_text if file.endswith(\".txt\")]\n",
        "# 檔案排序，需要考慮 檔案名稱長短不一的問題，問前是透過數字的處理\n",
        "# files_text.sort(key=lambda x:int(x[:-4]))\n",
        "if file2epub:\n",
        "  mdfiles=[ itm for itm in files_text]\n",
        "  os.system(\"pandoc -o \\\"../{}.epub\\\" title.txt {}\".format(title,\" \".join(mdfiles)))\n",
        "  from google.colab import files\n",
        "  files.download('../{}.epub'.format(title))\n",
        "  pass\n",
        "else:\n",
        "  with open(f\"../{output_name}.txt\",\"w\",encoding='utf-8') as f:\n",
        "    for file in files_text[::-1]:\n",
        "      with open(file,\"r\") as f2:\n",
        "        f.write(f2.read())\n",
        "  from google.colab import files\n",
        "  files.download('../{}.txt'.format(output_name))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "zXHImWy6FhSW"
      },
      "outputs": [],
      "source": [
        "#@title 飄天文學\n",
        "#@markdown 還在修正的程式，可以直接從這一個區塊執行\n",
        "def check_package(itm):\n",
        "  import importlib\n",
        "  try:\n",
        "    importlib.import_module(itm)\n",
        "    print(f\"{itm} 套件已經安裝\")\n",
        "  except ImportError:\n",
        "    print(f\"{itm} 套件尚未安裝，正在安裝中...\")\n",
        "    !pip install {itm}\n",
        "\n",
        "\n",
        "check_package(\"inlp\")\n",
        "\n",
        "\n",
        "import requests\n",
        "import os,re\n",
        "from bs4 import BeautifulSoup\n",
        "import concurrent.futures\n",
        "from inlp.convert import chinese\n",
        "\n",
        "try:\n",
        "  os.mkdir(\"/content/tmp\")\n",
        "except:\n",
        "  print(\"目錄已存在\")\n",
        "os.chdir(\"/content/tmp\")\n",
        "os.system(\"rm -fr *\")\n",
        "\n",
        "\n",
        "def get_html(urls):\n",
        "  [title,art_url]=urls\n",
        "  art_id=art_url[art_url.rfind(\"/\")+1:-5]\n",
        "  # print(art_url)\n",
        "  req=requests.get(art_url)\n",
        "  # req.encoding=\"gbk\"\n",
        "  soup=BeautifulSoup(req.text)\n",
        "  # content=soup.find('div', {'id': 'content', 'class': 'fonts_mesne'})\n",
        "\n",
        "  print(art_id)\n",
        "\n",
        "\n",
        "  # Find all the siblings of the table element up to the center element\n",
        "  content = []\n",
        "  content=soup.find(name=\"div\",id='booktxt')\n",
        "  # print(content)\n",
        "  # print(soup)\n",
        "  for tag in content.find_all(['div', 'h1','table','script','center']):\n",
        "    tag.extract()\n",
        "\n",
        "  text=f\"# {title}\\n\\n\"\n",
        "  context=str(content).replace(\"\\n\",\"\").replace(\"\\r\",\"\")\n",
        "  context=context.replace(\"\\xa0\\xa0\\xa0\\xa0\",\"\")\n",
        "  context=re.sub('<div class=\"ad_content\".*?</div>','',context)\n",
        "  context=context.replace(\"<br/>\",\"\\n\").replace(\"</p>\",\"\\n\")\n",
        "  context=re.sub('<.*?>',\"\",context).split(\"\\n\")\n",
        "  context=[itm.strip() for itm in context if len(itm)>0]\n",
        "  text+=\"\\n\\n\".join(context)+\"\\n\\n\"\n",
        "  text=chinese.s2t(text)\n",
        "\n",
        "\n",
        "\n",
        "  with open(f\"{art_id}.txt\",mode=\"w\",encoding=\"utf-8\") as f:\n",
        "    f.write(text)\n",
        "\n",
        "\n",
        "#@markdown 書籍目錄網址\n",
        "url=\"https://www.piaotianba.com/ptoewgi\" #@param {type:'string'}\n",
        "title=\"先修諸天萬道再修仙\" #@param {type:\"string\"}\n",
        "author=\"門前鴨\" #@param {type:\"string\"}\n",
        "#@markdown 打勾，將會直接變成 epub\n",
        "file2epub = True #@param {type:\"boolean\"}\n",
        "\n",
        "# 標題設定義\n",
        "YAML=f'''---\n",
        "title: {title}\n",
        "author: {author}\n",
        "language: zh-Hant\n",
        "---'''\n",
        "\n",
        "with open(\"title.txt\",mode=\"w\",encoding='utf-8') as f:\n",
        "  f.write(YAML)\n",
        "\n",
        "# sites=url\n",
        "sites=url[:url.rfind(\"/\")]\n",
        "\n",
        "reg=requests.get(url)\n",
        "#reg.encoding = 'gbk'\n",
        "# soup=BeautifulSoup(reg.text,\"html.parser\")\n",
        "soup=BeautifulSoup(reg.text)\n",
        "\n",
        "\n",
        "# articles=soup.find(name=\"div\",class_=\"centent\").find_all(\"a\")\n",
        "articles=soup.find(name=\"dl\",id=\"newlist\").find_all(\"a\")\n",
        "\n",
        "links=[]\n",
        "# len(articles)\n",
        "for i in articles:\n",
        "  href=i.get(\"href\")\n",
        "  links.append([chinese.s2t(i.text),f\"{sites}/{href}\"])\n",
        "files_text=[link[1][link[1].rfind(\"/\")+1:link[1].rfind(\".\")]+\".txt\" for link in links]\n",
        "\n",
        "# # 暫時無法使用，目前 colab 只要開放 2 個執行緖、只能運作 60 秒\n",
        "# # # 同時建立及啟用10個執行緒\n",
        "# # with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
        "# #     executor.map(get_html, links)\n",
        "for link in links:\n",
        "  get_html(link)\n",
        "\n",
        "output_name=title\n",
        "# files_text=os.listdir()\n",
        "# files_text=[file for file in files_text if file.endswith(\".txt\")]\n",
        "# 檔案排序，需要考慮 檔案名稱長短不一的問題，問前是透過數字的處理\n",
        "# files_text.sort(key=lambda x:int(x[:-4]))\n",
        "if file2epub:\n",
        "  mdfiles=[ itm for itm in files_text]\n",
        "  os.system(\"pandoc -o \\\"../{}.epub\\\" title.txt {}\".format(title,\" \".join(mdfiles)))\n",
        "  from google.colab import files\n",
        "  files.download('../{}.epub'.format(title))\n",
        "  pass\n",
        "else:\n",
        "  with open(f\"../{output_name}.txt\",\"w\",encoding='utf-8') as f:\n",
        "    for file in files_text:\n",
        "      with open(file,\"r\") as f2:\n",
        "        f.write(f2.read())\n",
        "  from google.colab import files\n",
        "  files.download('../{}.txt'.format(output_name))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "Sj1k1RHcre_O",
        "outputId": "6f436a2e-4f10-4a28-a6ca-aed0138b7d4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inlp 套件尚未安裝，正在安裝中...\n",
            "Collecting inlp\n",
            "  Using cached iNLP-0.0.2-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting simhash (from inlp)\n",
            "  Downloading simhash-2.1.2-py3-none-any.whl.metadata (382 bytes)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from simhash->inlp) (1.26.4)\n",
            "Downloading iNLP-0.0.2-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading simhash-2.1.2-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: simhash, inlp\n",
            "Successfully installed inlp-0.0.2 simhash-2.1.2\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'find_all'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-2fa1a90ecc46>\u001b[0m in \u001b[0;36m<cell line: 88>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m \u001b[0marticles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"div\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"catalog\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"a\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0mlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
          ]
        }
      ],
      "source": [
        "#@title 69書吧\n",
        "#@markdown 還在修正的程式，可以直接從這一個區塊執行\n",
        "def check_package(itm):\n",
        "  import importlib\n",
        "  try:\n",
        "    importlib.import_module(itm)\n",
        "    print(f\"{itm} 套件已經安裝\")\n",
        "  except ImportError:\n",
        "    print(f\"{itm} 套件尚未安裝，正在安裝中...\")\n",
        "    !pip install {itm}\n",
        "\n",
        "\n",
        "check_package(\"inlp\")\n",
        "\n",
        "\n",
        "import requests\n",
        "import os,re\n",
        "from bs4 import BeautifulSoup\n",
        "import concurrent.futures\n",
        "from inlp.convert import chinese\n",
        "\n",
        "try:\n",
        "  os.mkdir(\"/content/tmp\")\n",
        "except:\n",
        "  print(\"目錄已存在\")\n",
        "os.chdir(\"/content/tmp\")\n",
        "os.system(\"rm -fr *\")\n",
        "\n",
        "\n",
        "def get_html(urls):\n",
        "  [title,art_url]=urls\n",
        "  art_id=art_url[art_url.rfind(\"/\")+1:]\n",
        "  # print(art_url)\n",
        "  req=requests.get(art_url)\n",
        "  req.encoding=\"gbk\"\n",
        "  soup=BeautifulSoup(req.text)\n",
        "  print(art_id)\n",
        "\n",
        "  # Find all the siblings of the table element up to the center element\n",
        "  content = []\n",
        "  content=soup.find(name=\"div\",class_=\"txtnav\")\n",
        "\n",
        "  for tag in content.find_all(['div','h1','script','center']):\n",
        "    tag.extract()\n",
        "\n",
        "\n",
        "  text=f\"# {title}\\n\\n\"\n",
        "  context=str(content).replace(\"\\n\",\"\").replace(\"\\r\",\"\")\n",
        "  context=context.replace(\"\\xa0\\xa0\\xa0\\xa0\",\"\")\n",
        "  context=re.sub('<div class=\"ad_content\".*?</div>','',context)\n",
        "  context=context.replace(\"<br/>\",\"\\n\").replace(\"</p>\",\"\\n\")\n",
        "  context=re.sub('<.*?>',\"\",context).split(\"\\n\")\n",
        "  context=[itm.strip() for itm in context if len(itm)>0]\n",
        "  text+=\"\\n\\n\".join(context)+\"\\n\\n\"\n",
        "  text=chinese.s2t(text)\n",
        "\n",
        "  # print(text)\n",
        "\n",
        "  with open(f\"{art_id}.txt\",mode=\"w\",encoding=\"utf-8\") as f:\n",
        "    f.write(text)\n",
        "\n",
        "\n",
        "#@markdown 書籍目錄網址\n",
        "url=\"https://69shuba.cx/book/58865/\" #@param {type:'string'}\n",
        "title=\"暴富很難？我的超市通古今！\" #@param {type:\"string\"}\n",
        "author=\"琴止\" #@param {type:\"string\"}\n",
        "#@markdown 打勾，將會直接變成 epub\n",
        "file2epub = True #@param {type:\"boolean\"}\n",
        "\n",
        "# 標題設定義\n",
        "YAML=f'''---\n",
        "title: {title}\n",
        "author: {author}\n",
        "language: zh-Hant\n",
        "---'''\n",
        "\n",
        "with open(\"title.txt\",mode=\"w\",encoding='utf-8') as f:\n",
        "  f.write(YAML)\n",
        "\n",
        "sites=url\n",
        "# sites=url[:url.rfind(\"/\")]\n",
        "\n",
        "reg=requests.get(url)\n",
        "reg.encoding = 'gbk'\n",
        "# soup=BeautifulSoup(reg.text,\"html.parser\")\n",
        "soup=BeautifulSoup(reg.text)\n",
        "\n",
        "articles=soup.find(name=\"div\",id=\"catalog\").find_all(\"a\")\n",
        "\n",
        "links=[]\n",
        "# # len(articles)\n",
        "for i in articles:\n",
        "  href=i.get(\"href\")\n",
        "  links.append([chinese.s2t(i.text),href])\n",
        "\n",
        "files_text=[link[1][link[1].rfind(\"/\")+1:]+\".txt\" for link in links]\n",
        "\n",
        "# 暫時無法使用，目前 colab 只要開放 2 個執行緖、只能運作 60 秒\n",
        "# # 同時建立及啟用10個執行緒\n",
        "# with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
        "#     executor.map(get_html, links)\n",
        "for link in links:\n",
        "  get_html(link)\n",
        "\n",
        "output_name=title\n",
        "# files_text=os.listdir()\n",
        "# files_text=[file for file in files_text if file.endswith(\".txt\")]\n",
        "# 檔案排序，需要考慮 檔案名稱長短不一的問題，問前是透過數字的處理\n",
        "# files_text.sort(key=lambda x:int(x[:-4]))\n",
        "if file2epub:\n",
        "  mdfiles=[ itm for itm in files_text]\n",
        "  os.system(\"pandoc -o \\\"../{}.epub\\\" title.txt {}\".format(title,\" \".join(mdfiles)))\n",
        "  from google.colab import files\n",
        "  files.download('../{}.epub'.format(title))\n",
        "  pass\n",
        "else:\n",
        "  with open(f\"../{output_name}.txt\",\"w\",encoding='utf-8') as f:\n",
        "    for file in files_text:\n",
        "      with open(file,\"r\") as f2:\n",
        "        f.write(f2.read())\n",
        "  from google.colab import files\n",
        "  files.download('../{}.txt'.format(output_name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "lkVncEDb8tXj"
      },
      "outputs": [],
      "source": [
        "#@title 笔趣阁(bqg9527.net/)\n",
        "#@markdown 還在修正的程式，可以直接從這一個區塊執行\n",
        "\n",
        "def check_package(itm):\n",
        "  import importlib\n",
        "  try:\n",
        "    importlib.import_module(itm)\n",
        "    print(f\"{itm} 套件已經安裝\")\n",
        "  except ImportError:\n",
        "    print(f\"{itm} 套件尚未安裝，正在安裝中...\")\n",
        "    !pip install {itm}\n",
        "\n",
        "\n",
        "check_package(\"inlp\")\n",
        "\n",
        "import requests\n",
        "import os,re\n",
        "from bs4 import BeautifulSoup\n",
        "import concurrent.futures\n",
        "from inlp.convert import chinese\n",
        "\n",
        "\n",
        "try:\n",
        "  os.mkdir(\"/content/tmp\")\n",
        "except:\n",
        "  print(\"目錄已存在\")\n",
        "os.chdir(\"/content/tmp\")\n",
        "os.system(\"rm -fr *\")\n",
        "\n",
        "\n",
        "def get_html(urls):\n",
        "  [title,art_url]=urls\n",
        "  art_id=art_url[art_url.rfind(\"/\")+1:-5]\n",
        "\n",
        "  soup=BeautifulSoup(requests.get(art_url).text)\n",
        "  # content=soup.find(name=\"div\",id='nr1')\n",
        "  content=soup.find(name=\"div\",id=\"content\")\n",
        "  print(art_id)\n",
        "  # print(soup)\n",
        "\n",
        "  text=f\"# {title}\\n\\n\"\n",
        "  context=str(content).replace(\"\\n\",\"\").replace(\"\\r\",\"\")\n",
        "  context=context.replace(\"\\xa0\\xa0\\xa0\\xa0\",\"\")\n",
        "  context=re.sub('<div class=\"ad_content\".*?</div>','',context)\n",
        "  context=context.replace(\"<br/>\",\"\\n\").replace(\"</p>\",\"\\n\")\n",
        "  context=re.sub('<.*?>',\"\",context).split(\"\\n\")\n",
        "  context=[itm.strip() for itm in context if len(itm)>0]\n",
        "  text+=\"\\n\\n\".join(context)+\"\\n\\n\"\n",
        "  text=chinese.s2t(text)\n",
        "\n",
        "  # print(text)\n",
        "\n",
        "\n",
        "  with open(f\"{art_id}.txt\",mode=\"w\",encoding=\"utf-8\") as f:\n",
        "    f.write(text)\n",
        "\n",
        "\n",
        "#@markdown 書籍目錄網址\n",
        "url=\"https://www.bqg9527.net/book/342930/\" #@param {type:'string'}\n",
        "title=\"我出錢你出命，我倆一起神經病\" #@param {type:\"string\"}\n",
        "author=\"我煞費苦心\" #@param {type:\"string\"}\n",
        "#@markdown 打勾，將會直接變成 epub\n",
        "file2epub = True #@param {type:\"boolean\"}\n",
        "\n",
        "# 標題設定義\n",
        "YAML=f'''---\n",
        "title: {title}\n",
        "author: {author}\n",
        "language: zh-Hant\n",
        "---'''\n",
        "\n",
        "with open(\"title.txt\",mode=\"w\",encoding='utf-8') as f:\n",
        "  f.write(YAML)\n",
        "\n",
        "# sites=url[:url.find(\"/\",8)+1]\n",
        "# sites=url[:url.rfind(\"/\")]\n",
        "sites=url\n",
        "reg=requests.get(url)\n",
        "# soup=BeautifulSoup(reg.text,\"html.parser\")\n",
        "soup=BeautifulSoup(reg.text)\n",
        "# articles=soup.find_all(name=\"ul\",class_=\"chapter\")[1].find_all(\"a\")\n",
        "articles=soup.find(name=\"div\",id=\"list\").find_all(\"dt\")[1].find_next_siblings('dd')\n",
        "\n",
        "# second_dt=soup.find_all('dt')[1]\n",
        "# dd_tags = second_dt.find_next_siblings('dd')\n",
        "\n",
        "links=[]\n",
        "# len(articles)\n",
        "for itm in articles:\n",
        "  i=itm.find(\"a\")\n",
        "  href=i.get(\"href\")\n",
        "  if href[-4:]!= \"html\":\n",
        "    continue\n",
        "  links.append([i.text,f\"{sites}{href}\"])\n",
        "\n",
        "\n",
        "files_text=[link[1][link[1].rfind(\"/\")+1:link[1].rfind(\".\")]+\".txt\" for link in links]\n",
        "# print(links[0])\n",
        "# print(files_text[0])\n",
        "# get_html(links[0])\n",
        "\n",
        "# # 暫時無法使用，目前 colab 只要開放 2 個執行緖、只能運作 60 秒\n",
        "# # # 同時建立及啟用10個執行緒\n",
        "with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
        "    executor.map(get_html, links)\n",
        "\n",
        "# for link in links:\n",
        "  # get_html(link)\n",
        "\n",
        "# output_name=soup.find(\"h1\").getText()\n",
        "output_name=title\n",
        "# files_text=os.listdir()\n",
        "# files_text=[file for file in files_text if file.endswith(\".txt\")]\n",
        "# 檔案排序，需要考慮 檔案名稱長短不一的問題，問前是透過數字的處理\n",
        "# files_text.sort(key=lambda x:int(x[:-4]))\n",
        "if file2epub:\n",
        "  mdfiles=[ itm for itm in files_text]\n",
        "  os.system(\"pandoc -o \\\"../{}.epub\\\" title.txt {}\".format(title,\" \".join(mdfiles)))\n",
        "  from google.colab import files\n",
        "  files.download('../{}.epub'.format(title))\n",
        "  pass\n",
        "else:\n",
        "  with open(f\"../{output_name}.txt\",\"w\",encoding='utf-8') as f:\n",
        "    for file in files_text[::-1]:\n",
        "      with open(file,\"r\") as f2:\n",
        "        f.write(f2.read())\n",
        "  from google.colab import files\n",
        "  files.download('../{}.txt'.format(output_name))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Afz6HuGb85ZL"
      },
      "outputs": [],
      "source": [
        "#@title 笔趣阁(p2wt)\n",
        "#@markdown 還在修正的程式，可以直接從這一個區塊執行\n",
        "\n",
        "def check_package(itm):\n",
        "  import importlib\n",
        "  try:\n",
        "    importlib.import_module(itm)\n",
        "    print(f\"{itm} 套件已經安裝\")\n",
        "  except ImportError:\n",
        "    print(f\"{itm} 套件尚未安裝，正在安裝中...\")\n",
        "    !pip install {itm}\n",
        "\n",
        "\n",
        "check_package(\"inlp\")\n",
        "\n",
        "import requests\n",
        "import os,re\n",
        "from bs4 import BeautifulSoup\n",
        "import concurrent.futures\n",
        "from inlp.convert import chinese\n",
        "\n",
        "\n",
        "try:\n",
        "  os.mkdir(\"/content/tmp\")\n",
        "except:\n",
        "  print(\"目錄已存在\")\n",
        "os.chdir(\"/content/tmp\")\n",
        "os.system(\"rm -fr *\")\n",
        "\n",
        "\n",
        "def get_html(urls):\n",
        "  [title,art_url]=urls\n",
        "  art_id=art_url[art_url.rfind(\"/\")+1:-5]\n",
        "  reg=requests.get(art_url)\n",
        "  reg.encoding=\"utf-8\"\n",
        "  soup=BeautifulSoup(reg.text)\n",
        "  content=soup.find(name=\"div\",id='chaptercontent')\n",
        "  print(art_id)\n",
        "\n",
        "  text=f\"# {title}\\n\\n\"\n",
        "  context=str(content).replace(\"\\n\",\"\").replace(\"\\r\",\"\")\n",
        "  context=context.replace(\"\\xa0\\xa0\\xa0\\xa0\",\"\")\n",
        "  context=re.sub('<div class=\"ad_content\".*?</div>','',context)\n",
        "  context=context.replace(\"<br/>\",\"\\n\").replace(\"</p>\",\"\\n\")\n",
        "  context=re.sub('<.*?>',\"\",context).split(\"\\n\")\n",
        "  context=[itm.strip() for itm in context if len(itm)>0]\n",
        "  text+=\"\\n\\n\".join(context)+\"\\n\\n\"\n",
        "  text=chinese.s2t(text)\n",
        "\n",
        "  with open(f\"{art_id}.txt\",mode=\"w\",encoding=\"utf-8\") as f:\n",
        "    f.write(text)\n",
        "\n",
        "\n",
        "\n",
        "def toEpub():\n",
        "# files_text=os.listdir()\n",
        "# files_text=[file for file in files_text if file.endswith(\".txt\")]\n",
        "# 檔案排序，需要考慮 檔案名稱長短不一的問題，問前是透過數字的處理\n",
        "# files_text.sort(key=lambda x:int(x[:-4]))\n",
        "  if file2epub:\n",
        "    mdfiles=[ itm for itm in files_text]\n",
        "    os.system(\"pandoc -o \\\"../{}.epub\\\" title.txt {}\".format(title,\" \".join(mdfiles)))\n",
        "    from google.colab import files\n",
        "    files.download('../{}.epub'.format(title))\n",
        "    pass\n",
        "  else:\n",
        "    with open(f\"../{output_name}.txt\",\"w\",encoding='utf-8') as f:\n",
        "      for file in files_text[::-1]:\n",
        "        with open(file,\"r\") as f2:\n",
        "          f.write(f2.read())\n",
        "    from google.colab import files\n",
        "    files.download('../{}.txt'.format(output_name))\n",
        "\n",
        "\n",
        "#@markdown 書籍目錄網址\n",
        "url=\"https://m.bqg9527.net/book/480/\" #@param {type:'string'}\n",
        "title=\"\\u7D66\\u4E0D\\u8D77\\u5F69\\u79AE\\uFF0C\\u53EA\\u597D\\u5A36\\u4E86\\u9B54\\u9580\\u8056\\u5973\" #@param {type:\"string\"}\n",
        "author=\"\\u5149\\u5F71\" #@param {type:\"string\"}\n",
        "#@markdown 打勾，將會直接變成 epub\n",
        "file2epub = True #@param {type:\"boolean\"}\n",
        "\n",
        "# 標題設定義\n",
        "YAML=f'''---\n",
        "title: {title}\n",
        "author: {author}\n",
        "language: zh-Hant\n",
        "---'''\n",
        "\n",
        "with open(\"title.txt\",mode=\"w\",encoding='utf-8') as f:\n",
        "  f.write(YAML)\n",
        "\n",
        "sites=url[:url.find(\"/\",8)]\n",
        "# sites=url[:url.rfind(\"/\")], 取得 sites\n",
        "\n",
        "reg=requests.get(url)\n",
        "# soup=BeautifulSoup(reg.text,\"html.parser\")\n",
        "reg.encoding=\"utf-8\"\n",
        "soup=BeautifulSoup(reg.text)\n",
        "output_name=soup.find(\"h1\").getText()\n",
        "articles=soup.find(name=\"div\",class_=\"listmain\").find_all(\"a\")\n",
        "\n",
        "\n",
        "links=[]\n",
        "# len(articles)\n",
        "for i in articles:\n",
        "  href=i.get(\"href\")\n",
        "  if href[-4:]!= \"html\":\n",
        "    continue\n",
        "  links.append([i.text,f\"{sites}{href}\"])\n",
        "\n",
        "files_text=[link[1][link[1].rfind(\"/\")+1:link[1].rfind(\".\")]+\".txt\" for link in links]\n",
        "\n",
        "\n",
        "# 暫時無法使用，目前 colab 只要開放 2 個執行緖、只能運作 60 秒\n",
        "# # 同時建立及啟用10個執行緒\n",
        "# with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
        "#     executor.map(get_html, links)\n",
        "for link in links:\n",
        "  get_html(link)\n",
        "\n",
        "\n",
        "toEpub()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "roy0O2ZJnAvz"
      },
      "outputs": [],
      "source": [
        "#@title 愛下電子書(撰寫中)\n",
        "\n",
        "\n",
        "import requests\n",
        "import os,re\n",
        "from bs4 import BeautifulSoup\n",
        "import concurrent.futures\n",
        "\n",
        "try:\n",
        "  os.mkdir(\"/content/tmp\")\n",
        "except:\n",
        "  print(\"目錄已存在\")\n",
        "os.chdir(\"/content/tmp\")\n",
        "os.system(\"rm -fr *\")\n",
        "\n",
        "\n",
        "\n",
        "def get_html(urls):\n",
        "  [title,art_url]=urls\n",
        "  art_id=art_url[art_url.rfind(\"/\")+1:-5]\n",
        "\n",
        "  soup=BeautifulSoup(requests.get(art_url).text)\n",
        "  # content=soup.find(name=\"div\",id='nr1')\n",
        "  content=soup.find(\"section\")\n",
        "\n",
        "  print(art_id)\n",
        "\n",
        "  text=f\"# {title}\\n\\n\"\n",
        "  context=str(content).replace(\"\\n\",\"\").replace(\"\\r\",\"\")\n",
        "  context=context.replace(\"\\xa0\\xa0\\xa0\\xa0\",\"\")\n",
        "  context=re.sub('<div class=\"ad_content\".*?</div>','',context)\n",
        "  context=re.sub('<script.*</script>','',context)\n",
        "  context=context.replace(\"<br/>\",\"\\n\").replace(\"</p>\",\"\\n\")\n",
        "  context=re.sub('<.*?>',\"\",context).split(\"\\n\")\n",
        "  context=[itm.strip() for itm in context if len(itm)>0]\n",
        "  text+=\"\\n\\n\".join(context)+\"\\n\\n\"\n",
        "  # text=chinese.s2t(text)\n",
        "\n",
        "  # print(text)\n",
        "\n",
        "\n",
        "  with open(f\"{art_id}.txt\",mode=\"w\",encoding=\"utf-8\") as f:\n",
        "    f.write(text)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#@markdown 書籍目錄網址\n",
        "url=\"https://ixdzs8.tw/read/381070/p484.html\" #@param {type:'string'}\n",
        "title=\"豪門梟士\" #@param {type:\"string\"}\n",
        "author=\"雲山風海\" #@param {type:\"string\"}\n",
        "#@markdown 打勾，將會直接變成 epub\n",
        "file2epub = True #@param {type:\"boolean\"}\n",
        "\n",
        "# 標題設定義\n",
        "YAML=f'''---\n",
        "title: {title}\n",
        "author: {author}\n",
        "language: zh-Hant\n",
        "---'''\n",
        "\n",
        "with open(\"title.txt\",mode=\"w\",encoding='utf-8') as f:\n",
        "  f.write(YAML)\n",
        "\n",
        "sites=url[:url.find(\"/\",8)]\n",
        "# sites=url[:url.rfind(\"/\")]\n",
        "\n",
        "# reg=requests.get(url)\n",
        "soup=BeautifulSoup(data)\n",
        "# articles=soup.find(name=\"div\",id=\"readerlists\").find_all(\"a\")\n",
        "articles=soup.find_all(\"a\")\n",
        "# print(articles)\n",
        "\n",
        "\n",
        "links=[]\n",
        "\n",
        "for i in articles:\n",
        "  href=i.get(\"href\")\n",
        "  itmTitle=i.getText()\n",
        "  links.append([itmTitle,f\"{sites}{href}\"])\n",
        "# links.sort(key=lambda x: x[1])\n",
        "\n",
        "\n",
        "files_text=[link[1][link[1].rfind(\"/\")+1:-5]+\".txt\" for link in links]\n",
        "# links[:4]\n",
        "\n",
        "# 暫時無法使用，目前 colab 只要開放 2 個執行緖、只能運作 60 秒\n",
        "# # 同時建立及啟用10個執行緒\n",
        "# with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
        "#     executor.map(get_html, links)\n",
        "\n",
        "for link in links:\n",
        "  get_html(link)\n",
        "\n",
        "output_name=title\n",
        "listfiles=os.listdir()\n",
        "mdfiles=[ itm for itm in files_text if itm in listfiles]\n",
        "# 檔案排序，需要考慮 檔案名稱長短不一的問題，問前是透過數字的處理\n",
        "# files_text.sort(key=lambda x:int(x[:-4]))\n",
        "if file2epub:\n",
        "  # mdfiles=[ itm for itm in files_text if itm in listfiles]\n",
        "  os.system(\"pandoc -o \\\"../{}.epub\\\" title.txt {}\".format(title,\" \".join(mdfiles)))\n",
        "  from google.colab import files\n",
        "    files.download('../{}.epub'.format(title))\n",
        "  pass\n",
        "else:\n",
        "  with open(f\"../{output_name}.txt\",\"w\",encoding='utf-8') as f:\n",
        "    for file in mdfiles:\n",
        "      with open(file,\"r\") as f2:\n",
        "        f.write(f2.read())\n",
        "  from google.colab import files\n",
        "  files.download('../{}.txt'.format(output_name))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-2pn1oynXxC"
      },
      "outputs": [],
      "source": [
        "#@title 52書庫(www.52shuku.vip)\n",
        "#@markdown 還在修正的程式，可以直接從這一個區塊執行\n",
        "\n",
        "def check_package(itm):\n",
        "  import importlib\n",
        "  try:\n",
        "    importlib.import_module(itm)\n",
        "    print(f\"{itm} 套件已經安裝\")\n",
        "  except ImportError:\n",
        "    print(f\"{itm} 套件尚未安裝，正在安裝中...\")\n",
        "    !pip install {itm}\n",
        "\n",
        "\n",
        "check_package(\"inlp\")\n",
        "\n",
        "import requests\n",
        "import os,re\n",
        "from bs4 import BeautifulSoup\n",
        "import concurrent.futures\n",
        "from inlp.convert import chinese\n",
        "\n",
        "\n",
        "try:\n",
        "  os.mkdir(\"/content/tmp\")\n",
        "except:\n",
        "  print(\"目錄已存在\")\n",
        "os.chdir(\"/content/tmp\")\n",
        "os.system(\"rm -fr *\")\n",
        "\n",
        "\n",
        "def get_html(urls):\n",
        "  [title,art_url]=urls\n",
        "  art_id=art_url[art_url.rfind(\"/\")+1:-5]\n",
        "  # print(art_id)\n",
        "  # print(art_url)\n",
        "  reg=requests.get(art_url)\n",
        "  reg.encoding=\"utf-8\"\n",
        "  soup=BeautifulSoup(reg.text)\n",
        "  content=soup.find(name=\"div\",id='text')\n",
        "  print(art_id)\n",
        "\n",
        "  text=f\"# {title}\\n\\n\"\n",
        "  context=str(content).replace(\"\\n\",\"\").replace(\"\\r\",\"\")\n",
        "  context=context.replace(\"\\xa0\\xa0\\xa0\\xa0\",\"\")\n",
        "  context=re.sub('<div class=\"ad_content\".*?</div>','',context)\n",
        "  context=context.replace(\"<br/>\",\"\\n\").replace(\"</p>\",\"\\n\")\n",
        "  context=re.sub('<.*?>',\"\",context).split(\"\\n\")\n",
        "  context=[itm.strip() for itm in context if len(itm)>0]\n",
        "  text+=\"\\n\\n\".join(context)+\"\\n\\n\"\n",
        "  text=chinese.s2t(text)\n",
        "\n",
        "  # print(text)\n",
        "  with open(f\"{art_id}.txt\",mode=\"w\",encoding=\"utf-8\") as f:\n",
        "    f.write(text)\n",
        "\n",
        "\n",
        "#@markdown 書籍目錄網址\n",
        "url=\"https://www.52shuku.vip/yanqing/b/bjPGg.html\" #@param {type:'string'}\n",
        "title=\"\\u59D1\\u5A18\\u5979\\u7F8E\\u8C8C\\u5374\\u66B4\\u529B\" #@param {type:\"string\"}\n",
        "author=\"\\u4E60\\u6829\\u5112\\u751F\" #@param {type:\"string\"}\n",
        "#@markdown 打勾，將會直接變成 epub\n",
        "file2epub = True #@param {type:\"boolean\"}\n",
        "\n",
        "# 標題設定義\n",
        "YAML=f'''---\n",
        "title: {title}\n",
        "author: {author}\n",
        "language: zh-Hant\n",
        "---'''\n",
        "\n",
        "with open(\"title.txt\",mode=\"w\",encoding='utf-8') as f:\n",
        "  f.write(YAML)\n",
        "\n",
        "sites=url[:url.find(\"/\",8)]\n",
        "# sites=url[:url.rfind(\"/\")]\n",
        "\n",
        "reg=requests.get(url)\n",
        "reg.encoding=\"utf-8\"\n",
        "# soup=BeautifulSoup(reg.text,\"html.parser\")\n",
        "soup=BeautifulSoup(reg.text)\n",
        "output_name=soup.find(\"h1\").getText()\n",
        "articles=soup.find(name=\"ul\",class_=\"list\").find_all(\"a\")\n",
        "\n",
        "\n",
        "links=[]\n",
        "# len(articles)\n",
        "for i in articles:\n",
        "  href=i.get(\"href\")\n",
        "  if href[-4:]!= \"html\":\n",
        "    continue\n",
        "  links.append([i.text,f\"{href}\"])\n",
        "\n",
        "files_text=[link[1][link[1].rfind(\"/\")+1:link[1].rfind(\".\")]+\".txt\" for link in links]\n",
        "\n",
        "\n",
        "\n",
        "# 暫時無法使用，目前 colab 只要開放 2 個執行緖、只能運作 60 秒\n",
        "# # 同時建立及啟用10個執行緒\n",
        "# with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
        "#     executor.map(get_html, links)\n",
        "for link in links:\n",
        "  get_html(link)\n",
        "\n",
        "output_name=soup.find(\"h1\").getText()\n",
        "# files_text=os.listdir()\n",
        "# files_text=[file for file in files_text if file.endswith(\".txt\")]\n",
        "# 檔案排序，需要考慮 檔案名稱長短不一的問題，問前是透過數字的處理\n",
        "# files_text.sort(key=lambda x:int(x[:-4]))\n",
        "if file2epub:\n",
        "  mdfiles=[ itm for itm in files_text]\n",
        "  os.system(\"pandoc -o \\\"../{}.epub\\\" title.txt {}\".format(title,\" \".join(mdfiles)))\n",
        "  from google.colab import files\n",
        "  files.download('../{}.epub'.format(title))\n",
        "  pass\n",
        "else:\n",
        "  with open(f\"../{output_name}.txt\",\"w\",encoding='utf-8') as f:\n",
        "    for file in files_text[::-1]:\n",
        "      with open(file,\"r\") as f2:\n",
        "        f.write(f2.read())\n",
        "  from google.colab import files\n",
        "  files.download('../{}.txt'.format(output_name))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJOqF-eIbaCT"
      },
      "source": [
        "# 測試後，暫時沒有使用的程式碼區塊"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKVabV1-DNyF",
        "outputId": "7d3b5bda-b7d1-47b0-c830-d5ed944562d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Command executed successfully: \n"
          ]
        }
      ],
      "source": [
        "#@title 檢查 os 錯誤內容\n",
        "\n",
        "\n",
        "command = \"pandoc -o \\\"../{}.epub\\\" title.txt {}\".format(title,\" \".join(mdfiles))\n",
        "try:\n",
        "    result = subprocess.run(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
        "    if result.returncode != 0:\n",
        "        print(\"Error:\", result.stderr)\n",
        "    else:\n",
        "        print(\"Command executed successfully:\", result.stdout)\n",
        "except Exception as e:\n",
        "    print(\"An error occurred:\", str(e))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNxKvxlfhvO6"
      },
      "source": [
        "# 效率\n",
        "\n",
        "透過下述的方法，合併檔案，因為輸出檔需要被反覆的開始太多次，隨著檔案大小逐漸增加。讓效能下跌\n",
        "\n",
        "```python\n",
        "for file in files:\n",
        "  os.system(\"cat {}>> ../{}.txt\".format(file,output_name))\n",
        "```\n",
        "若改用下述的方法， output 檔，只需要開啟一次。可以大大縮短時間。\n",
        "\n",
        "```python\n",
        "with open(f\"../{output_name}.txt\",\"w\",encoding='utf-8') as f:\n",
        "  for file in files_text:\n",
        "    with open(file,\"r\") as f2:\n",
        "      f.write(f2.read())\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KCQpgIUtZ3M"
      },
      "source": [
        "# 參考資料"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4CCMkYp5OxmZ"
      },
      "outputs": [],
      "source": [
        "#@title 多執行序參考程式範例\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import concurrent.futures\n",
        "import requests\n",
        "import time\n",
        "\n",
        "\n",
        "def scrape(urls):\n",
        "\n",
        "    response = requests.get(urls)\n",
        "\n",
        "    soup = BeautifulSoup(response.content, \"lxml\")\n",
        "\n",
        "    # 爬取文章標題\n",
        "    titles = soup.find_all(\"h3\", {\"class\": \"post_title\"})\n",
        "\n",
        "    for title in titles:\n",
        "        print(title.getText().strip())\n",
        "\n",
        "    time.sleep(2)\n",
        "\n",
        "\n",
        "base_url = \"https://www.inside.com.tw/tag/AI\"\n",
        "urls = [f\"{base_url}?page={page}\" for page in range(1, 6)]  # 1~5頁的網址清單\n",
        "print(urls)\n",
        "start_time = time.time()  # 開始時間\n",
        "# scrape(urls)\n",
        "\n",
        "\n",
        "# 同時建立及啟用10個執行緒\n",
        "# with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
        "#     executor.map(scrape, urls)\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"{end_time - start_time} 秒爬取 {len(urls)} 頁的文章\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "11h2gvU2w0w-cWs1O2ciOwdgK4Wi6qndj",
      "authorship_tag": "ABX9TyOhWP8TOoOjVXwk6vdh8oWc",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}